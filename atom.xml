<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jinxi Blog</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-09-08T05:24:06.768Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Jinxi</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>信息、熵、KL散度、交叉熵</title>
    <link href="http://example.com/2021/09/07/%E4%BF%A1%E6%81%AF%E3%80%81%E7%86%B5%E3%80%81KL%E6%95%A3%E5%BA%A6%E3%80%81%E4%BA%A4%E5%8F%89%E7%86%B5/"/>
    <id>http://example.com/2021/09/07/%E4%BF%A1%E6%81%AF%E3%80%81%E7%86%B5%E3%80%81KL%E6%95%A3%E5%BA%A6%E3%80%81%E4%BA%A4%E5%8F%89%E7%86%B5/</id>
    <published>2021-09-07T13:22:56.000Z</published>
    <updated>2021-09-08T05:24:06.768Z</updated>
    
    <content type="html"><![CDATA[<p>本文介绍了什么是信息和熵，以及如何去计算信息量和熵的大小，KL散度的理解、交叉熵和KL散的关系。</p><span id="more"></span><h2 id="信息信息量熵">1. 信息、信息量、熵</h2><h3 id="什么是信息熵">1.1 什么是信息、熵</h3><p><strong>定义：</strong>当一件事情（宏观态）有多种可能情况（微观态）时，这件事情对观察者而言具体是那种情况的不确定性叫做<strong>熵</strong>。而能够消除该人对这件事情的不确定事物叫做<strong>信息</strong>。信息是从多个可能状态中确定实际状态所需的物理量，所以获取信息意味着消除熵。</p><p><strong>信息的本质</strong>是调整观察者对于一件事情（宏观态）的微观态判断的概率。</p><p>不能够消除某人对于某件事情不确定性的事物被称为<strong>噪音</strong>，<strong>数据=噪音+信息</strong>。</p><p><strong>信息、熵的性质</strong></p><ol type="1"><li>信息与传递它的媒介无关。</li><li>信息是相对于观察者已经对该件事情的实际了解程度而言的，是需要能调整观察者自己对于微观态判断的概率。</li><li>信息是客观的物理量，不随观察者的主观意识改变，只有确定了真正的实际情况时才是信息，虚假的不叫信息。</li><li>信息（熵）还是相对于某件事情而言的</li></ol><p><strong>概率和熵的区别</strong>：概率是某件事情（宏观态）某个可能情况（微观态）的确定性，而熵是某件事情（宏观态）到底是哪个情况（微观态）的不确定性。</p><h3 id="信息量的计算">1.2 信息量的计算</h3><p>信息是客观物理量，所以是可以被度量的，但是信息应该怎么度量呢？比如质量，我们找到一个参照物为1kg，其他物品和该参照物比较，有几个参照物的大小就是几千克。</p><p>测量信息也是一样，信息是消除不确定性，那么就选择一个事情的不确定性作为参照物。当参照事件是像抛硬币一样有两种等概率事件，这时测量单位就叫<strong>比特（bit）</strong>，然而信息不像是质量是线性增加的，而是指数增加。抛三次硬币的可能情况是<span class="math inline">\(2^3\)</span>而不是<span class="math inline">\(2\times3\)</span>，所以假设<span class="math inline">\(m\)</span>是被测事件等概率的情况个数，<span class="math inline">\(k\)</span>是参照事件的等概率情况个数，则需要的信息为<span class="math inline">\(\log_k m\)</span>。</p><p><strong>如果被测事件不是等概率的该如何计算信息呢？</strong></p><p>概率是<span class="math inline">\(\frac{1}{100}\)</span>，相当于从100个等概率情况中确定实际情况，概率的倒数等于等概率情况的个数，由此</p><p>可以得到<span class="math inline">\(I(x)=\log\frac{1}{P(x)}=-\log P(x)\)</span></p><h3 id="熵的计算">1.3 熵的计算</h3><p>用熵描述系统的不确定性，熵越大，信息量越大，不确定性越大。熵可以表示为期望信息总量，也可以理解为编码方案完美时，最短平均编码长度，计算公式：<span class="math inline">\(H(\mathrm{x})=\mathbb{E}_{\mathrm{x} \sim P}[I(x)]=-\mathbb{E}_{\mathrm{x} \sim P}[\log P(x)]=\sum_{x} P(x) \log P(x)\)</span></p><h2 id="kl-散度相对熵">2. KL 散度（相对熵）</h2><p><strong>定义：</strong>KL散度又称为相对熵，是两个几率分布P和Q差别的非对称性的度量。KL散度是用来度量使用基于Q的分布来编码服从P的分布的样本所需的额外的平均比特数。典型情况下，P表示数据的真实分布，Q表示数据的理论分布、估计的模型分布、或P的近似分布。</p><p>当我们知道真实的概率分布之后，可以给出最有效的编码。如果我们使用了不同于真实分布的概率分布，那么我们一定会损失编码效率，并且在传输时增加的平均额外信息量至少等于两个分布之间的KL散度。如果使用以2为底的对数计算，则KL散度表示信息损失的二进制位数。</p><p>KL散度表示用一个分布近似另一个分布时信息的损失量。</p><p>对于连续随机变量，分布P和Q的KL散度为：<span class="math inline">\(D_{\mathrm{KL}}(P \| Q)=\int_{-\infty}^{\infty} p(x) \ln \frac{p(x)}{q(x)} \mathrm{d} x\)</span></p><p>对于离散随机变量，分布P和Q的KL散度为：<span class="math inline">\(D_{\mathrm{KL}}(P \| Q)=-\sum_{i} P(i) \ln \frac{Q(i)}{P(i)}\)</span></p><p><strong>性质：</strong></p><p><strong>1、相对熵的值为非负值</strong></p><p><span class="math inline">\(KL(p||Q)\geq 0\)</span>，并且当且仅当<span class="math inline">\(p(x)=q(x)\)</span>时等号成立。证明略。</p><p><strong>2、KL散度不具有对称性</strong></p><p>因为<span class="math inline">\(D_{\mathrm{KL}}(P \| Q) \neq D_{\mathrm{KL}}(Q \| P)\)</span> ，所以KL散度并不是一个真正的度量或者距离函数。</p><p><strong>3、最小化KL散度等价于最大化似然函数</strong></p><p>假设我们用参数分布<span class="math inline">\(q(x|\theta)\)</span>来近似<span class="math inline">\(p(x)\)</span>，因为不知大概率分布<span class="math inline">\(p(x)\)</span>，我们观察服从<span class="math inline">\(p(x)\)</span>分布的有限数量的训练集合<span class="math inline">\({x_n}\)</span>。KL散度可以近似为 <span class="math inline">\(KL(p||q)\approx \frac{1}{N}\sum_{n=1}^N(-\ln q(x_n|\theta)+\ln p(x_n))\)</span>，最小化KL散度与<span class="math inline">\(\ln p(x_n)\)</span>无关，而<span class="math inline">\(\frac{1}{N}\sum_{n=1}^N(-\ln q(x_n|\theta)\)</span>是分布<span class="math inline">\(q(x|\theta )\)</span>的负对数似然函数。所以最小化KL散度等价于最大化似然函数。</p><p>KL散度可以理解为编码方案不一定完美时，平均编码长度相对于最小值的增加值。<span class="math inline">\(KL(p||q)=H(p,q)-H(p)\)</span></p><h2 id="交叉熵">3. 交叉熵</h2><p>在信息论中，基于相同事件测度的两个概率分布<span class="math inline">\(p\)</span>和<span class="math inline">\(q\)</span>的交叉熵是指，当基于一个“非自然”（相对于“真实”分布<span class="math inline">\(p\)</span>而言）的概率分布<span class="math inline">\(q\)</span>进行编码时，在事件集合中唯一标识一个事件所需要的平均比特数（bit）。交叉熵可以理解为编码方案不一定完美时（由于对概率分布的估计不一定正确），平均编码长度。所以交叉熵的计算公式为：<span class="math inline">\(H(p,q)=-\sum_i p_i\)</span></p><h2 id="reference">Reference</h2><p>[1] <a href="https://zh.wikipedia.org/wiki/相对熵">相对熵 - 维基百科，自由的百科全书 (wikipedia.org)</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/p/39682125">KL散度理解 - 知乎 (zhihu.com)</a></p><p>[3] <a href="https://zh.wikipedia.org/wiki/交叉熵">交叉熵 - 维基百科，自由的百科全书 (wikipedia.org)</a></p><p>[4] <a href="https://zhuanlan.zhihu.com/p/55599202">信息为什么还有单位，熵为什么用 log 来计算？ - 知乎 (zhihu.com)</a></p><p>[5] <a href="https://zhuanlan.zhihu.com/p/55459472">学习观10：老师，我没有传纸条作弊，我在学习信息论 - 知乎 (zhihu.com)</a></p><p>[6] <a href="https://www.jianshu.com/p/43318a3dc715">如何理解K-L散度（相对熵） - 简书 (jianshu.com)</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文介绍了什么是信息和熵，以及如何去计算信息量和熵的大小，KL散度的理解、交叉熵和KL散的关系。&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="学习笔记" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>markdown转为知乎文章</title>
    <link href="http://example.com/2021/09/02/markdown%E8%BD%AC%E4%B8%BA%E7%9F%A5%E4%B9%8E%E6%96%87%E7%AB%A0/"/>
    <id>http://example.com/2021/09/02/markdown%E8%BD%AC%E4%B8%BA%E7%9F%A5%E4%B9%8E%E6%96%87%E7%AB%A0/</id>
    <published>2021-09-03T01:31:03.000Z</published>
    <updated>2021-09-03T17:36:09.630Z</updated>
    
    <content type="html"><![CDATA[<p>最近为在知乎上写文章，markdown转到知乎公式$$环境不能很好的转换，在网上找到了一个方法很方便。</p><ol type="1"><li><p>打开<code>notepad++</code>编辑器，<code>ctrl+f</code>，切记打开<strong>正则表达式</strong>和<strong>匹配新行</strong>：</p></li><li>将文章公式中的$$替换，<code>\$\$\n*((.|\n)*?)\n*\$\$</code>替换为<code>\n&lt;img src=&quot;https://www.zhihu.com/equation?tex=\1&quot; alt=&quot;\1&quot; class=&quot;ee_img tr_noresize&quot; eeimg=&quot;1&quot;&gt;\n</code></li><li>将文章公式中的$替换：<code>\$\n*(.*?)\n*\$</code>替换为<code>\n&lt;img src=&quot;https://www.zhihu.com/equation?tex=\1&quot; alt=&quot;\1&quot; class=&quot;ee_img tr_noresize&quot; eeimg=&quot;1&quot;&gt;\n</code></li><li><p>知乎导入markdown文件</p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最近为在知乎上写文章，markdown转到知乎公式$$环境不能很好的转换，在网上找到了一个方法很方便。&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;打开&lt;code&gt;notepad++&lt;/code&gt;编辑器，&lt;code&gt;ctrl+f&lt;/code&gt;，切记打开&lt;stron</summary>
      
    
    
    
    <category term="杂记" scheme="http://example.com/categories/%E6%9D%82%E8%AE%B0/"/>
    
    
    <category term="学习笔记" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>David Silver Course《Lec08 整合学习与规划》</title>
    <link href="http://example.com/2021/09/02/notebook08-dyna/"/>
    <id>http://example.com/2021/09/02/notebook08-dyna/</id>
    <published>2021-09-02T20:19:27.000Z</published>
    <updated>2021-09-03T12:37:05.785Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一introduction">一、Introduction</h2><p>​ 多数强化学习问题可以通过表格式或基于近似函数来直接学习状态价值或策略函数，在这些学习方法中，个体并不试图去理解环境动力学。如果能建立一个较为准确地模拟环境动力学特征的模型或者问题的模型本身就类似于一些棋类游戏是明确或者简单的，个体就可以通过构建这样的模型来模拟其与环境的交互，这种依靠模型模拟而不实际与环境交互的过程类似于“<strong>思考</strong>”过程。通过思考，个体可以对问题进行规划、在与环境实际交互时搜索交互可能产生的各种后果并从中选择对个体有利的结果。<strong>这种思想可以广泛应用于规则简单、状态或结果复杂的强化学习问题中。</strong></p><span id="more"></span><p><img src="https://cdn.mathpix.com/snip/images/iv_y1opApBnKJ6pH9GNUZDte62ot_eriAGD21lcd38g.original.fullsize.png" /></p><p>​ 模型M是一个马尔科夫决策过程MDP<span class="math inline">\(&lt;S,A,P,R&gt;\)</span>的参数化形式，来描述状态转换和奖励函数。 <span class="math display">\[\begin{array}{l}S_{t+1} \sim P_{\eta}\left(S_{t+1} \mid S_{t}, A_{t}\right) \\R_{t+1}=R_{\eta}\left(R_{t+1} \mid S_{t}, A_{t}\right)\end{array}\]</span> ​ 通常<strong>假设状态转换和奖励之间条件独立</strong>： <span class="math display">\[\mathbb{P}\left[S_{t+1}, R_{t+1} \mid S_{t}, A_{t}\right]=\mathbb{P}\left[S_{t+1} \mid S_{t}, A_{t}\right] \mathbb{P}\left[R_{t+1} \mid S_{t}, A_{t}\right]\]</span></p><h2 id="二model-based-reinforcement-learning">二、Model-Based Reinforcement Learning</h2><h3 id="优缺点">2.1 优缺点</h3><p>优点：</p><ul><li>可以通过监督学习方法有效地学习模型</li><li>可以推理出模型不确定性</li></ul><p>缺点：</p><ul><li>首先学习一个模型，然后构造一个值函数，会有更大的误差。</li></ul><p>​ 模型学习是一个根据experience的监督学习问题，其中状态转换是概率分布问题，奖励是回归问题。</p><h3 id="查表法">2.2 查表法</h3><p>​ 计算公式如下： <span class="math display">\[\begin{aligned}\hat{\mathcal{P}}_{s, s^{\prime}}^{a} &amp;=\frac{1}{N(s, a)} \sum_{t=1}^{T} \mathbf{1}\left(S_{t}, A_{t}, S_{t+1}=s, a, s^{\prime}\right) \\\hat{\mathcal{R}}_{s}^{a} &amp;=\frac{1}{N(s, a)} \sum_{t=1}^{T} \mathbf{1}\left(S_{t}, A_{t}=s, a\right) R_{t}\end{aligned}\]</span></p><h3 id="planning-with-a-model">2.3 Planning with a Model</h3><p>方法：</p><ul><li>Value iteration</li><li>Policy iteration</li><li>Tree search</li></ul><h3 id="sample-based-planning">2.4 Sample-Based Planning</h3><p>​ 一个简单但是有效的planning方法，从model中采样experience，然后用Model-free的RL方法求解。</p><ul><li>Monte-Carlo control</li><li>Sarsa</li><li>Q-learning</li></ul><h2 id="三integrated-architectures">三、Integrated Architectures</h2><h3 id="dyna算法">3.1 Dyna算法</h3><p>Dyna算法思想</p><ul><li>Learn a model from real experience</li><li>Learn and plan value function (and/or policy) from real and simulated experience</li></ul><p><img src="https://cdn.mathpix.com/snip/images/kefHr2Oa71WxZx11KNQB0vLeAlan3QVU9Gc8LyEtMKA.original.fullsize.png" /></p><p>Dyna-Q算法流程：</p><p><img src="https://cdn.mathpix.com/snip/images/RmPTn14Mco6tQCKJTBHU0fVLzbX-1y6h4h4_nbL_1TA.original.fullsize.png" /></p><h2 id="四simulation-based-search">四、Simulation-Based Search</h2><h3 id="forward-search">4.1 Forward Search</h3><p>​ 不用求解全部的MDP，只需关注从此处开始的MDP分支。在强化学习中，基于模拟的搜索 (simulation-based search) 是一种前向搜索形式，它从当前时刻的状态开始，利用模型来模拟采样，构建一个关注短期未来的前向搜索树，将构建得到的搜 索树作为一个学习资源，使用不基于模型的强化学习方法来寻找当前状态下的最优策略 (图 8.3)。如果使用蒙特卡罗学习方法则称为蒙特卡罗搜索，如果使用 Sarsa 学习方法，则称为 TD 搜索。其中蒙特卡罗搜索又分为简单蒙特卡罗搜索和蒙特卡罗树搜索。</p><p>​ 对于一个模型 <span class="math inline">\(M_v\)</span> 和一个一致的模拟过程中使用的策略 <span class="math inline">\(\pi\)</span>，简单蒙特卡罗搜索在当前实际状态 <span class="math inline">\(s_t\)</span> 时会针对行为空间中的每一个行为 <span class="math inline">\(a\in A\)</span> 进行 <span class="math inline">\(K\)</span> 次的模拟采样： <span class="math display">\[\left\{s_{t}, a, R_{t+1}^{k}, S_{t+1}^{k}, A_{t+1}^{k}, \ldots, S_{T}^{k}\right\}_{k=1}^{K} \sim M_{v}, \pi\]</span> ​ 通过计算模拟采样得到的 k 个状态 st 时采取行为 s 的收获的平均值来估算该状态行为对的价值： <span class="math display">\[Q\left(s_{t}, a\right)=\frac{1}{K} \sum_{k=1}^{K} G_{t}\]</span></p><pre><code>    比较行为空间中所有行为 a 的价值，确定当前状态 $s_t$ 下与环境发生实际交互的行为$a_t$：  </code></pre><p><span class="math display">\[a_{t}=\underset{a \in \mathrm{A}}{\operatorname{argmax}} Q\left(s_{t}, a\right)\]</span></p><p>​ 简单蒙特卡罗搜索可以使用基于模拟的采样对当前模拟采样的策略进行评估，得到基于模拟采样的某状态行为对的价值，这个价值的估计同时还与每次采样的 K 值大小有关。在估算行为价值时，关注点在于从当前状态和行为对应的收获，并不关注模拟采样得到的一些中间状态和对应行为的价值。如果同时考虑模拟得到的中间状态和行为的价值，则可以考虑蒙特卡罗树搜索。</p><h3 id="蒙特卡罗树搜索">4.2 蒙特卡罗树搜索</h3><p>​ 蒙特卡罗树搜索 (Monte-Carlo tree search, MCTS) 在构建当前状态 <span class="math inline">\(s_t\)</span> 的基于模拟的前向搜索时，关注模拟采样中所经历的所有状态及对应的行为，以此构建一个搜索树。利用这颗搜索树不仅可以对当前模拟策略进行评估，还可以改善模拟策略。在使用蒙特卡罗树搜索进行模拟策略评估时，对于个体构建的模型 <span class="math inline">\(M_v\)</span> 和当前的模拟策略<span class="math inline">\(\pi\)</span>，在实际当前状态 <span class="math inline">\(s_t\)</span> 时模拟采样出 K 个完整状态序列：<br /><span class="math display">\[\left\{s_{t}, A_{t}^{k}, R_{t+1}^{k}, S_{t+1}^{k}, \ldots, S_{T}^{k}\right\}_{k=1}^{K} \sim M_{v}, \pi\]</span> ​ 构建一颗以状态 <span class="math inline">\(s_t\)</span> 为根节点包括所有已访问的状态和行为的搜索树，对树内的每一个状态行为对 <span class="math inline">\((s; a)\)</span> 使用该状态行为对的平均收获来估算其价值：<br /><span class="math display">\[Q(s, a)=\frac{1}{N(s, a)} \sum_{k=1}^{K} \sum_{u=t}^{T} 1\left(S_{u}, A_{u}=s, a\right) G_{u}\]</span> ​ 当搜索结束时，比较当前状态 <span class="math inline">\(s_t\)</span> 下行为空间 A 内的每一个行为的价值，从中选择最大价值对应的行为 <span class="math inline">\(a_t\)</span> 作为当前状态 <span class="math inline">\(s_t\)</span> 时个体与环境实际交互的行为。</p><p>​ 比较简单蒙特卡罗搜索和蒙特卡罗树搜索，可以看出两者之间的区别在于前者针对当前状态 <span class="math inline">\(s_t\)</span> 时每一个可能的行为都进行相同数量的采样，而后者则是根据模拟策略进行一定次数的采样。此外，蒙特卡罗树搜索会对模拟采样产生的状态行为对进行计数，并计算其收获，根据这两个数据来计算模拟采样对应的状态行为对价值。比较两者之间的差别可以看出，如果问题的行为空间规模很大，那么使用蒙特卡罗树搜索比简单蒙特卡罗搜索要更实际可行。在蒙特卡罗树搜索中，搜索树的广度和深度是伴随着模拟采样的增多而逐渐增多的。在构建这个搜索树的过程中，搜索树内状态行为对的价值也在不停的更新，利用这些更新的价值信息可以使得在每模拟采样得到一个完整的状态序列后都可以一定程度地改进模拟策略。通常蒙特卡罗树搜索的策略分为两个阶段：</p><ol type="1"><li>树内策略 (tree policy)：为当模拟采样得到的状态存在于当前的搜索树中时适用的策略，该策略。树内策略可以使 ϵ-贪婪策略，随着模拟的进行可以得到持续改善；</li><li>默认策略 (default policy)：当前状态不在搜索树内时，使用默认策略来完成整个状态序列的采样，并把当前状态纳入到搜索树中。默认策略可以使随机策略或基于某目标价值函数的策略。</li></ol><p>​ 随着不断地重复模拟，状态行为对的价值将得到持续地得到评估。同时搜索树的深度和广度将得到扩展，策略也不断得到改善。蒙特卡罗树搜索较为抽象，本章暂时介绍到这里，在第十章介绍 AlphaZero 算法时会利用五子棋实例详细讲解蒙特卡罗树搜索的过程细节。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;一introduction&quot;&gt;一、Introduction&lt;/h2&gt;
&lt;p&gt;​ 多数强化学习问题可以通过表格式或基于近似函数来直接学习状态价值或策略函数，在这些学习方法中，个体并不试图去理解环境动力学。如果能建立一个较为准确地模拟环境动力学特征的模型或者问题的模型本身就类似于一些棋类游戏是明确或者简单的，个体就可以通过构建这样的模型来模拟其与环境的交互，这种依靠模型模拟而不实际与环境交互的过程类似于“&lt;strong&gt;思考&lt;/strong&gt;”过程。通过思考，个体可以对问题进行规划、在与环境实际交互时搜索交互可能产生的各种后果并从中选择对个体有利的结果。&lt;strong&gt;这种思想可以广泛应用于规则简单、状态或结果复杂的强化学习问题中。&lt;/strong&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Reinforcement Learning" scheme="http://example.com/categories/Reinforcement-Learning/"/>
    
    
    <category term="学习笔记" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>David Silver Course《Lec07 策略梯度方法》</title>
    <link href="http://example.com/2021/09/02/notebook07-pg/"/>
    <id>http://example.com/2021/09/02/notebook07-pg/</id>
    <published>2021-09-02T20:19:15.000Z</published>
    <updated>2021-09-03T12:37:43.554Z</updated>
    
    <content type="html"><![CDATA[<h2 id="introduction">1. Introduction</h2><p>在这一章节我们学习直接参数化策略（policy），如下公式。</p><span id="more"></span><p><span class="math display">\[\pi_{\theta}(s, a)=\mathbb{P}[a \mid s, \theta]\]</span></p><h3 id="value-based-policy-based-actor-critic的关系">1.1 Value-Based 、Policy-Based 、Actor-Critic的关系：</h3><p><img src="https://cdn.mathpix.com/snip/images/0FfJ8juRIklj-NoCIdzG8mJXY8EAv-Ma9MPFVogtlqs.original.fullsize.png" /></p><h3 id="value-based-policy-based的优缺点">1.2 Value-Based 、Policy-Based的优缺点</h3><p><strong>Policy-Based Advantages：</strong></p><ul><li>具有更好的收敛性能</li><li>对高维或连续的行为空间也有效。（单纯基于价值函数近似的方法无法解决<strong>连续行为空间</strong>的问题）</li><li>可以学习随机策略（剪刀石头布）</li></ul><p><strong>Policy-Based Disadvantages:</strong></p><ul><li>通常收敛到局部最优，而不是全局最优</li><li>评估一个政策通常是低效的和高方差的</li></ul><p>​ 基于价值的学习对应的最优策略通常是确定性策略，因为其是从众多行为价值中选择一个最大价值的行为，而有些问题的最优策略却是随机策略，这种情况下同样是无法通过基于价值的学习来求解的。可以看出，基于价值的强化学习虽然能出色地解决很多问题，但面对行为<strong>空间连续、观测受限、随机策略</strong>的学习等问题时仍然显得力不从心。此时基于策略的学习是解决这类问题的一个新的途径。</p><h3 id="目标函数">1.3 目标函数</h3><p>1、初始状态价值（start value），<span class="math inline">\(G_1\)</span>表示第一步s1的收获。 <span class="math display">\[J_{1}(\theta)=V_{\pi_{\theta}}\left(s_{1}\right)=\mathbb{E}_{\pi_{\theta}}\left[G_{1}\right]\]</span></p><p>2、平均价值（average value） <span class="math display">\[J_{\text {av } V}(\theta)=\sum_{s} d^{\pi_{\theta}}(s) V^{\pi_{\theta}}(s)\]</span></p><p>3、每一时间步的平均奖励（average reward per time-step） <span class="math display">\[J_{a v R}(\theta)=\sum_{s} d^{\pi_{\theta}}(s) \sum_{a} \pi_{\theta}(s, a) \mathcal{R}_{s}^{a}\]</span></p><p>​ <span class="math inline">\(d^{\pi_{\theta}}(s)\)</span>是基于策略<span class="math inline">\(\pi_\theta\)</span>生成的马尔科夫链关于状态的静态分布，并不是指起始状态的分布，而是整体看来基于该策略状态出现概率的分布。对于目标函数的优化有不基于梯度和基于梯度的优化方法，本章关注于基于梯度的优化方法，并且会探索序列的结构。</p><h2 id="finite-difference-policy-gradient-有限差分策略梯度">2. Finite Difference Policy Gradient 有限差分策略梯度</h2><h3 id="梯度下降求解">2.1 梯度下降求解</h3><p>​ 采用梯度下降方法求解，则：</p><p><span class="math display">\[\nabla_{\theta} J(\theta)=\left(\begin{array}{c}\frac{\partial J(\theta)}{\partial \theta_{1}} \\\vdots \\\frac{\partial J(\theta)}{\partial \theta_{n}}\end{array}\right)\]</span></p><p><span class="math display">\[\Delta \theta=\alpha \nabla_{\theta} J(\theta)\]</span></p><p>​ 其中<span class="math inline">\(\nabla_{\theta} J(\theta)\)</span>为策略梯度，<span class="math inline">\(\alpha\)</span>是步长。我们需要评估<span class="math inline">\(\pi_\theta(s,a)\)</span>的策略梯度。对于每一个维度<span class="math inline">\(k\in[1,n]\)</span>，评估方式如下，这种方法简单，有噪音，但有时有效。 <span class="math display">\[\frac{\partial J(\theta)}{\partial \theta_{k}} \approx \frac{J\left(\theta+\epsilon u_{k}\right)-J(\theta)}{\epsilon}\]</span></p><h3 id="score-function">2.2 Score Function</h3><p>​ 现在我们来计算策略梯度，首先我们假设<span class="math inline">\(\pi_\theta\)</span>当非零时是可导的，并且我们知道$<em>{} </em>{}(s, a) $。</p><p><span class="math display">\[\begin{aligned}\nabla_{\theta} \pi_{\theta}(s, a) &amp;=\pi_{\theta}(s, a) \frac{\nabla_{\theta} \pi_{\theta}(s, a)}{\pi_{\theta}(s, a)} \\&amp;=\pi_{\theta}(s, a) \nabla_{\theta} \log \pi_{\theta}(s, a)\end{aligned}\]</span></p><p>​ 其中<span class="math inline">\(\nabla_{\theta} \log \pi_{\theta}(s, a)\)</span>为<strong>score function</strong>。</p><h4 id="softmax-策略">2.2.1 Softmax 策略</h4><p>​ Softmax 策略是应用于<strong>离散行为空间</strong>的一种常用策略。该策略使用描述状态和行为的特征<span class="math inline">\(\phi(s, a)\)</span>与参数<span class="math inline">\(\theta\)</span>的线性组合来权衡一个行为发生的几率：</p><p><span class="math display">\[\pi_{\theta} \propto e^{\phi(s, a)^{T} \theta}\]</span> ​ 对应的分值函数为：</p><p><span class="math display">\[\nabla_{\theta} \log \pi_{\theta}(s, a)=\phi(s, a)-\mathbb{E}_{\pi_{\theta}}[\phi(s, \cdot)]\]</span></p><h4 id="高斯策略">2.2.2 高斯策略</h4><p>​ 高斯策略是应用于<strong>连续行为空间</strong>的一种常用策略。该策略对应的行为从高斯分布<span class="math inline">\(\mathbb{N}\left(\mu(s), \sigma^{2}\right)\)</span>中产生。其均值 <span class="math inline">\(\mu(s)=\phi(s)^{T} \theta\)</span>。高斯策略对应的分值函数为:</p><p><span class="math display">\[\nabla_{\theta} \log \pi_{\theta}(s, a)=\frac{(a-\mu(s)) \phi(s)}{\sigma^{2}}\]</span></p><p>###　2.3 One-Step MDPs</p><p>​ 假设现在有一个单步马尔科夫决策过程，对应的强化学习问题是个体与环境每产生一个行为交互一次即得到一个即时奖励<span class="math inline">\(r=R_{s,a}\)</span>，并形成一个完整的状态序列。目标函数为： <span class="math display">\[\begin{aligned}J(\theta) &amp;=\mathbb{E}_{\pi_{\theta}}[r] \\&amp;=\sum_{s \in S} d(s) \sum_{a \in A} \pi_{\theta}(s, a) R_{s, a}\end{aligned}\]</span> ​</p><p>​ 目标函数的梯度为： <span class="math display">\[\begin{aligned}\nabla_{\theta} J(\theta) &amp;=\sum_{s \in S} d(s) \sum_{a \in A} \nabla_{\theta} \pi_{\theta}(s, a) R_{s, a} \\&amp;=\sum_{s \in S} d(s) \sum_{a \in A} \pi_{\theta}(s, a) \nabla_{\theta} \log \pi_{\theta}(s, a) R_{s, a} \\&amp;=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) r\right]\end{aligned}\]</span></p><h3 id="policy-gradient-theorem-策略梯度定理">2.4 Policy Gradient Theorem 策略梯度定理</h3><p>​ 策略梯度定理扩展到多步的MDP，将即时奖励r替换为长期价值<span class="math inline">\(Q^\pi(s,a)\)</span>。</p><p>​ 存在如下的策略梯度定理：对于任何可微的策略函数<span class="math inline">\(\pi_\theta(s,a)\)</span> 以及三种策略目标函数<span class="math inline">\(J_1,J_{avV},\frac{1}{1-\gamma}J_{avR}\)</span>中的任意一种来说，策略目标函数的梯度 (策略梯度) 都可以写成用分值函数表示的形式： <span class="math display">\[\nabla_{\theta} J(\theta)=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) Q_{\pi_{\theta}}(s, a)\right]\]</span></p><h2 id="monte-carlo-policy-gradient-蒙特卡洛策略梯度">3. Monte-Carlo Policy Gradient 蒙特卡洛策略梯度</h2><p>​ 该算法实际应用不多，主要是由于其需要完整的状态序列来计算收获，同时用收获来代替行为价值也存在较高的变异性，导致许多次的参数更新的方向有可能不是真正策略梯度的方向。其中的<span class="math inline">\(v_t\)</span>为累积回报，即为之前的<span class="math inline">\(G_t\)</span>。</p><p><img src="https://cdn.mathpix.com/snip/images/aqC54CyoGbckr5RZ1X7853uRQ7ZMtt00g6x_OanWVTM.original.fullsize.png" style="zoom:67%;" /></p><h2 id="actor-critic-policy-gradient">4. Actor-Critic Policy Gradient</h2><h3 id="qac算法">4.1 QAC算法</h3><pre><code>    蒙特卡洛算法有很高的方差，所以提出了Actor-Critic算法。Actor-Critic 算法包含一个策略函数和行为价值函数。其中策略函数充当演员 (Actor), 生成行为与环境交互；行为价值函数充当(Critic)，负责评价演员的表现，并指导演员的后续行为动作。Critic 的行为价值函数是基于策略$\pi_\theta$的一个近似：</code></pre><p><span class="math display">\[Q_{w}(s, a) \approx Q_{\pi_{\theta}}(s, a)\]</span></p><p>​ critic是要解决策略评估问题，这个问题在前几周讨论过，有（1）Monte-Carlo，（2）Temporal-Difference，（3）TD（<span class="math inline">\(\lambda\)</span>）</p><p>​ 假设：用线性价值函数来近似，critic根据TD(0)更新参数，Actor根据策略梯度更新参数，如下为QAC算法：</p><p><img src="https://cdn.mathpix.com/snip/images/-6gfKOsHU-Ys3Ksea1vz5wqLcGcCWY-RbaGCmEmxu_M.original.fullsize.png" /></p><h3 id="compatible-function-approximation-相容函数逼近定理">4.2 Compatible Function Approximation 相容函数逼近定理</h3><p>​ 简单的 QAC 算法虽然不需要完整的状态序列，但是由于引入的 Critic 仍然是一个近似价值函数，存在着引入偏差的可能性，不过当价值函数接受的输入的特征和函数近似方式足够幸运时，可以避免这种偏差而完全遵循策略梯度的方向。</p><p>​ <strong>定理</strong>：如果下面两个条件满足：</p><ol type="1"><li><p>近似价值函数的梯度与分值函数的梯度相同，即： <span class="math display">\[\nabla_{w} Q_{w}(s, a)=\nabla_{\theta} \log \pi_{\theta}(s, a)\]</span></p></li><li><p>近似价值函数的参数 w 能够最小化 <span class="math display">\[\epsilon=\mathbb{E}_{\pi_{\theta}}\left[\left(Q_{\pi_{\theta}}(s, a)-Q_{w}(s, a)\right)^{2}\right]\]</span></p></li></ol><p>​ 如果满足上面的两个条件，那么策略梯度 ∇ θ J(θ) 是准确的，即: <span class="math display">\[\nabla_{\theta} J(\theta)=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) Q_{w}(s, a)\right]\]</span></p><p><strong>证明：</strong></p><p><img src="https://cdn.mathpix.com/snip/images/E5XRR7ui4761T_J35jRnXcQZgirxTdUn29xB5EAky5w.original.fullsize.png" /></p><h3 id="reducing-variance-using-a-baseline">4.3 Reducing Variance Using a Baseline</h3><p>​ 我们从策略梯度中减去一个基线函数B(s)，可以在不改变期望的情况下减少方差，因为如下的公式为0 。 <span class="math display">\[\begin{aligned}\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) B(s)\right] &amp;=\sum_{s \in \mathcal{S}} d^{\pi_{\theta}}(s) \sum_{a} \nabla_{\theta} \pi_{\theta}(s, a) B(s) \\&amp;=\sum_{s \in \mathcal{S}} d^{\pi_{\theta}} B(s) \nabla_{\theta} \sum_{a \in \mathcal{A}} \pi_{\theta}(s, a) \\&amp;=0\end{aligned}\]</span> ​</p><p>​ 一个好的基线是状态值函数<span class="math inline">\(B(s)=V^{\pi_{\theta}}(s)\)</span>，则策略梯度改写为如下的，使得方差减小。 <span class="math display">\[\begin{aligned}A^{\pi \theta}(s, a) &amp;=Q^{\pi_{\theta}}(s, a)-V^{\pi_{\theta}(s)} \\\nabla_{\theta} J(\theta) &amp;=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) A^{\pi_{\theta}}(s, a)\right]\end{aligned}\]</span></p><p>​ 对于<span class="math inline">\(V^{\pi_\theta}(s)\)</span>的TD error，是对优势函数的无偏估计。 <span class="math display">\[\delta^{\pi_{\theta}}=r+\gamma V^{\pi_{\theta}}\left(s^{\prime}\right)-V^{\pi_{\theta}}(s)\]</span></p><p><span class="math display">\[\begin{aligned}\mathbb{E}_{\pi_{\theta}}\left[\delta^{\pi_{\theta}} \mid s, a\right] &amp;=\mathbb{E}_{\pi_{\theta}}\left[r+\gamma V^{\pi_{\theta}}\left(s^{\prime}\right) \mid s, a\right]-V^{\pi_{\theta}}(s) \\&amp;=Q^{\pi_{\theta}}(s, a)-V^{\pi_{\theta}}(s) \\&amp;=A^{\pi_{\theta}}(s, a)\end{aligned}\]</span></p><p>​ 所以可以计算如下策略梯度： <span class="math display">\[\nabla_{\theta} J(\theta)=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) \delta^{\pi_{\theta}}\right]\]</span> ​ 可以用参数来估计TD error，只需要一组参数v 。 <span class="math display">\[\delta_{v}=r+\gamma V_{v}\left(s^{\prime}\right)-V_{v}(s)\]</span></p><h3 id="policy-gradient-with-eligibility-traces">4.4 Policy Gradient with Eligibility Traces</h3><p>​ 可以像后向<span class="math inline">\(TD(\lambda)\)</span>一样引入eligibility traces 。 <span class="math display">\[\begin{aligned}\delta &amp;=r_{t+1}+\gamma V_{v}\left(s_{t+1}\right)-V_{v}\left(s_{t}\right) \\e_{t+1} &amp;=\lambda e_{t}+\nabla_{\theta} \log \pi_{\theta}(s, a) \\\Delta \theta &amp;=\alpha \delta e_{t}\end{aligned}\]</span></p><h3 id="natural-policy-gradient">4.5 Natural Policy Gradient</h3><p><span class="math display">\[\nabla_{\theta}^{n a t} \pi_{\theta}(s, a)=G_{\theta}^{-1} \nabla_{\theta} \pi_{\theta}(s, a)\]</span></p><p>​ Gθ is the Fisher information matrix ： <span class="math display">\[G_{\theta}=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) \nabla_{\theta} \log \pi_{\theta}(s, a)^{T}\right]\]</span></p><h3 id="natural-actor-critic">4.6 Natural Actor-Critic</h3><p>​ Using compatible function approximation ， <span class="math display">\[\nabla_{w} A_{w}(s, a)=\nabla_{\theta} \log \pi_{\theta}(s, a)\]</span> ​ So the natural policy gradient simplifies , <span class="math display">\[\begin{aligned}\nabla_{\theta} J(\theta) &amp;=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) A^{\pi_{\theta}}(s, a)\right] \\&amp;=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(s, a) \nabla_{\theta} \log \pi_{\theta}(s, a)^{T} w\right] \\&amp;=G_{\theta} w \\\nabla_{\theta}^{n a t} f(\theta) &amp;=w\end{aligned}\]</span></p><h2 id="深度确定性策略梯度-ddpg-算法">5. 深度确定性策略梯度 (DDPG) 算法</h2><p>​ 深度确定性策略梯度算法是使用深度学习技术、同时基于 Actor-Critic 算法的确定性策略算法。该算法中的 Actor 和 Critic 都使用深度神经网络来建立近似函数。由于该算法可以直接从Actor 的策略生成确定的行为而不需要依据行为的概率分布进行采样而被称为确定性策略。该算法在学习阶段通过在确定性的行为基础上增加一个噪声函数而实现在确定性行为周围的小范围内探索。此外，该算法还为 Actor 和 Critic 网络各备份了一套参数用来计算行为价值的期待值以更稳定地提升 Critic 的策略指导水平。使用备份参数的网络称为目标网络，其对应的参数每次更新的幅度很小。另一套参数对应的 Actor 和 Critic 则用来生成实际交互的行为以及计算相应的策略梯度，这一套参数每学习一次就更新一次。这种双参数设置的目的是为了减少因近似数据的引导而发生不收敛的情形。这四个网络具体使用的情景为：</p><ol type="1"><li>Actor 网络：根据当前状态 s0 生成的探索或不探索的具体行为 a0；</li><li>Target Actor 网络：根据环境给出的后续状态 s1 生成预估价值用到的 a1；</li><li>Critic 网络：计算状态 s0 和生成的行为 a0 对应的行为价值；</li><li>Target Critic 网络：根据后续状态 s1,a1 生成用来计算目标价值 y = Q(s0; a0) 的 Q′(s1; a1)；</li></ol><p>​ DDPG 算法表现出色，能较为稳定地解决连续行为空间下强化学习问题，其具体流程如下所示。</p><p><img src="https://cdn.mathpix.com/snip/images/WvvGSnOpYIeCrTKXV2x-Y_NyA8Zgz13Mf9l1uw2wvUA.original.fullsize.png" /></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;introduction&quot;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;在这一章节我们学习直接参数化策略（policy），如下公式。&lt;/p&gt;</summary>
    
    
    
    <category term="Reinforcement Learning" scheme="http://example.com/categories/Reinforcement-Learning/"/>
    
    
    <category term="学习笔记" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>David Silver Course《Lec06 价值函数的近似表示》</title>
    <link href="http://example.com/2021/09/02/notebook06-FA/"/>
    <id>http://example.com/2021/09/02/notebook06-FA/</id>
    <published>2021-09-02T20:19:04.000Z</published>
    <updated>2021-09-03T12:37:38.112Z</updated>
    
    <content type="html"><![CDATA[<h2 id="introduction">1. Introduction</h2><p>​ 本章解决状态数量多和连续状态的强化学习问题。<strong>通过参数w的函数来近似状态价值函数和状态行为对价值函数，在MC或者TD算法中通过更新w参数来实现</strong>。</p><span id="more"></span><p>​ 三种构建近似函数的方法：</p><p><img src="https://cdn.mathpix.com/snip/images/7GnJItI_y-ybf-jgkX6VAFbitVNrvzKof99ry-EFX2s.original.fullsize.png" /></p><p>​ 需要一个训练方法适合于非独立同分布的数据。</p><h2 id="incremental-methods">2. Incremental Methods</h2><h3 id="梯度下降">2.1 梯度下降</h3><p>略</p><h3 id="线性函数近似">2.2 线性函数近似</h3><p>略</p><h3 id="目标函数">2.3 目标函数</h3><p>​ 对于<strong>MC学习</strong>：</p><p><span class="math display">\[\begin{array}{c}J(w)=\frac{1}{2 M} \sum_{t=1}^{M}\left[G_{t}-\hat{V}\left(S_{t}, w\right)\right]^{2} \\J(w)=\frac{1}{2 M} \sum_{t=1}^{M}\left[G_{t}-\hat{Q}\left(S_{t}, A_{t}, w\right)\right]^{2}\end{array}\]</span> ​ 对于 <strong>TD(0)</strong> 和<strong>反向认识 TD(λ)</strong> 学习 ：</p><p><span class="math display">\[\begin{array}{c}J(w)=\frac{1}{2 M} \sum_{t=1}^{M}\left[\left(R_{t}+\gamma \hat{V}\left(S_{t}^{\prime}, w\right)\right)-\hat{V}\left(S_{t}, w\right)\right]^{2} \\J(w)=\frac{1}{2 M} \sum_{t=1}^{M}\left[\left(R_{t}+\gamma \hat{Q}\left(S_{t}^{\prime}, A_{t}^{\prime}, w\right)\right)-\hat{Q}\left(S_{t}, A_{t}, w\right)\right]^{2}\end{array}\]</span> ​ <strong>前向认识 TD(λ)</strong> 学习 ：</p><p><span class="math display">\[\begin{array}{c}J(w)=\frac{1}{2 M} \sum_{t=1}^{M}\left[G_{t}^{\lambda}-\hat{V}\left(S_{t}, w\right)\right]^{2} \\J(w)=\frac{1}{2 M} \sum_{t=1}^{M}\left[q_{t}^{\lambda}-\hat{Q}\left(S_{t}, A_{t}, w\right)\right]^{2}\end{array}\]</span></p><h2 id="batch-methods">3. Batch Methods</h2><h3 id="dqn算法">3.1 DQN算法</h3><p>​ DQN 算法主要使用<strong>经历回放 (experience replay) 来实现价值函数的收敛</strong>。其具体做法为：个体能记住既往的状态转换经历，对于每一个完整状态序列里的每一次状态转换，依据当前状态的<span class="math inline">\(s_t\)</span>价值以 ϵ-贪婪策略选择一个行为<span class="math inline">\(a_t\)</span>，执行该行为得到奖励<span class="math inline">\(r_{t+1}\)</span>和下一个状态<span class="math inline">\(s_{t+1}\)</span> ，将得到的状态转换存储至记忆中，当记忆中存储的容量足够大时，随机从记忆力提取一定数量的状态转换，用状态转换中下一状态来计算当前状态的目标价值，使用公式 (6.4) 计算目标价值与网络输出价值之间的均方差代价，使用小块梯度下降算法更新网络的参数。</p><p><img src="https://cdn.mathpix.com/snip/images/mCGcoEKCBkRRu_LY4glh8pDQC0NolS7FalfwpiZFsho.original.fullsize.png" /></p><h3 id="ddqn算法">3.2 DDQN算法</h3><p>​ DQN 算法在深度强化学习领域取得了不俗的成绩，不过其并不能保证一直收敛，研究表明这种估计目标价值的算法过于乐观的高估了一些情况下的行为价值，导致算法会将次优行为价值一致认为最优行为价值，最终不能收敛至最佳价值函数。一种使用双价值网络的 DDQN(double deep Q network) 被认为较好地解决了这个问题。该算法使用两个架构相同的近似价值函数，其中一个用来根据策略生成交互行为并随时频繁参数 (<span class="math inline">\(\theta\)</span>)，另一个则用来生成目标价值, 其参数 (<span class="math inline">\(\theta^-\)</span>) 每隔一定的周期进行更新。该算法绝大多数流程与 DQN 算法一样，只是在更新目标价值时使用公式：</p><p><span class="math display">\[Q_{\text {target}}\left(S_{t}, A_{t}\right)=R_{t}+\gamma Q\left(S_{t}^{\prime}, \max _{a^{\prime}} Q\left(S_{t}^{\prime}, a^{\prime} ; \theta\right) ; \theta^{-}\right)\]</span> ​ 该式表明，DDQN 在生成目标价值时使用了生成交互行为并频繁更新参数的价值网络<span class="math inline">\(Q(\theta)\)</span>，在这个价值网络中挑选状态<span class="math inline">\(S&#39;\)</span>下最大价值对应的行为<span class="math inline">\(A&#39;_t\)</span> ，随后再用状态行为对<span class="math inline">\((S&#39;_t,A&#39;_t)\)</span>代入目标价值网络<span class="math inline">\(Q(\theta^-)\)</span>得出目标价值。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;introduction&quot;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;​ 本章解决状态数量多和连续状态的强化学习问题。&lt;strong&gt;通过参数w的函数来近似状态价值函数和状态行为对价值函数，在MC或者TD算法中通过更新w参数来实现&lt;/strong&gt;。&lt;/p&gt;</summary>
    
    
    
    <category term="Reinforcement Learning" scheme="http://example.com/categories/Reinforcement-Learning/"/>
    
    
    <category term="学习笔记" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>David Silver Course《Lec05 不基于模型的控制》</title>
    <link href="http://example.com/2021/09/02/notebook05-control/"/>
    <id>http://example.com/2021/09/02/notebook05-control/</id>
    <published>2021-09-02T20:18:52.000Z</published>
    <updated>2021-09-03T12:37:33.250Z</updated>
    
    <content type="html"><![CDATA[<h2 id="introduction">1. Introduction</h2><ul><li><p>行为策略：指导个体产生与环境进行实际交互行为的策略。</p></li><li><p>目标策略：评价状态或行为价值的策略或者待优化的策略。</p></li><li><p>现时策略学习：个体在学习过程中优化的策略与自己的行为策略是同一个策略。</p></li><li><p>借鉴策略学习：个体在学习过程中优化的策略与自己的行为策略是不同的策略。</p></li></ul><span id="more"></span><h2 id="on-policy-monte-carlo-control">2. On-Policy Monte-Carlo Control</h2><p>​ 在不基于模型的控制时，我们将无法通过分析、比较基于状态的价值来改善贪婪策略，这是因为基于状态价值的贪婪策略的改善需要知晓状态间转移概率。我们无法事先知道这些状态之间在不同行为下的转移概率，因而无法基于状态价值来改善我们的贪婪策略。</p><h3 id="ϵ-greedy-exploration-ϵ--贪婪策略">2.1 ϵ-Greedy Exploration ϵ- 贪婪策略</h3><p><span class="math display">\[\pi(a | s)=\left\{\begin{array}{ll}\epsilon / m+1-\epsilon &amp; \text { 如果 } a^{*}=\underset{a \in A}{\operatorname{argmax}} Q(s, a) \\\epsilon / m &amp; \text { 其它情况 }\end{array}\right.\]</span></p><h3 id="monte-carlo-control-现时策略蒙特卡罗控制">2.2 Monte-Carlo Control 现时策略蒙特卡罗控制</h3><p>​ <strong>现时策略蒙特卡罗控制</strong>：<u>通过 ϵ-贪婪策略采样一个或多个完整的状态序列后，平均得出某一状态行为对的价值，并持续进行策略的评估和改善。通常可以在仅得到一个完整状态序列后就进行一次策略迭代以加速迭代过程。</u></p><p>​ <strong>理论基础</strong>：</p><p>​ <strong>GLIE</strong>(greedy in the Limit with Infnite Exploration) ：它包含两层意思，一是所有的状态行为对会被无限次探索 ；二是另外随着采样趋向无穷多，策略收敛至一个贪婪策略 。存在如下的定理：<strong>GLIE 蒙特卡洛控制能收敛至最优的状态行为价值函数。</strong>如果在使用 ϵ-贪婪策略时，能令 ϵ 随采样次数的无限增加而趋向于 0 就符合 GLIE。</p><p>​ <strong>流程</strong>：</p><ol type="1"><li><p>基于给定策略 π，采样第 k 个完整的状态序列 。</p></li><li><p>对于该状态序列里出现的每一状态行为对 <span class="math inline">\((S_t,A_t)\)</span>，更新其计数 N 和行为价值函数 Q</p><p><span class="math display">\[\begin{array}{c}N\left(S_{t}, A_{t}\right) \leftarrow N\left(S_{t}, A_{t}\right)+1 \\Q\left(S_{t}, A_{t}\right) \leftarrow Q\left(S_{t}, A_{t}\right)+\frac{1}{N\left(S_{t}, A_{t}\right)}\left(G_{t}-Q\left(S_{t}, A_{t}\right)\right)\end{array}\]</span></p></li><li><p>基于新的行为价值函数 Q 以如下方式改善策略</p><p><span class="math display">\[\begin{array}{c}\epsilon \leftarrow 1 / k \\\pi \leftarrow \epsilon-g \operatorname{reed} y(Q)\end{array}\]</span></p></li></ol><h2 id="on-policy-temporal-difference-learning-现时策略时序差分控制">3. On-Policy Temporal-Difference Learning 现时策略时序差分控制</h2><h3 id="sarsa-算法">3.1 Sarsa 算法</h3><p>​ <strong>Sarsa算法</strong>：<u>针对一个状态 S，个体通过行为策略产生一个行为 A，执行该行为进而产生一个状态行为对 (S,A)，环境收到个体的行为后会告诉个体即时奖励R 以及后续进入的状态 S’；个体在状态 S’ 时遵循当前的行为策略产生一个新行为 A’，个体此时并不执行该行为，而是通过行为价值函数得到后一个状态行为对 (S’,A’) 的价值，利用这个新的价值和即时奖励 R 来更新前一个状态行为对 (S,A) 的价值。</u></p><p>​ <strong>迭代公式</strong>：</p><p><span class="math display">\[Q(S, A) \leftarrow Q(S, A)+\alpha\left(R+\gamma Q\left(S^{\prime}, A^{\prime}\right)-Q(S, A)\right)\]</span> ​ <u>Sarsa算法流程</u>：参数 α 是学习速率参数， γ 是衰减因子。</p><p><img src="https://cdn.mathpix.com/snip/images/AaX0bwTi_Dkiw2nOl-75XSfR_wpnTDNldwjBoguHKfc.original.fullsize.png" /></p><p>​ 当行为策略满足前文所述的GLIE 特性同时学习速率参数 α 满足如下时，Sarsa 算法将收敛至最优策略和最优价值函数。</p><p><span class="math display">\[\sum_{t=1}^{\infty} \alpha_{t}=\infty, \text { 且 } \sum_{t=1}^{\infty} \alpha_{t}^{2}&lt;\infty\]</span></p><h3 id="sarsaλ-算法">3.2 Sarsa(λ) 算法</h3><p>​ 定义 <strong>n-步 Q 收获 (Q-return)</strong> 为：</p><p><span class="math display">\[q_{t}^{(n)}=R_{t+1}+\gamma R_{t+2}+\vdots+\gamma^{n-1} R_{t+n}+\gamma^{n} Q\left(S_{t+n}, A_{t+n}\right)\]</span> ​ <strong>更新公式</strong>如下：</p><p><span class="math display">\[Q\left(S_{t}, A_{t}\right) \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left(q_{t}^{(n)}-Q\left(S_{t}, A_{t}\right)\right)\]</span> ​ 类似于 TD(λ), 可以给 n-步 Q 收获中的每一步收获分配一个权重，并按权重对每一步 Q 收获求和：</p><p><span class="math display">\[q_{t}^{\lambda}=(1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} q_{t}^{(n)}\]</span> ​ 则：</p><p><span class="math display">\[Q\left(S_{t}, A_{t}\right) \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left(q_{t}^{(\lambda)}-Q\left(S_{t}, A_{t}\right)\right)\]</span> ​ 反向认识：</p><p><span class="math display">\[\begin{array}{c}E_{0}(s, a)=0 \\E_{t}(s, a)=\gamma \lambda E_{t-1}(s, a)+1\left(S_{t}=s, A_{t}=a\right), \gamma, \lambda \in[0,1]\end{array}\]</span></p><p><span class="math display">\[\begin{array}{c}\left.\delta_{t}=R_{t+1}+\gamma Q\left(S_{t+1}, A_{t+1}\right)-Q\left(S_{t}, A_{t}\right)\right) \\Q(s, a) \leftarrow Q(s, a)+\alpha \delta_{t} E_{t}(s, a)\end{array}\]</span></p><p>​ <strong><span class="math inline">\(Sarsa(\lambda)\)</span>算法流程</strong>：<img src="https://cdn.mathpix.com/snip/images/uDQmE7QZKegTDuVdF-rTTxggQp_AJdQLdvEA8Kh0fpg.original.fullsize.png" /></p><h2 id="off-policy-learning">4. Off-Policy Learning</h2><h3 id="借鉴策略-q-学习算法">4.1 借鉴策略 Q 学习算法</h3><p>​ <strong>借鉴策略学习</strong> (off-policy learning) 中产生指导自身行为的策略<span class="math inline">\(\mu(a|s)\)</span>与评价策略<span class="math inline">\(\pi(a|s)\)</span>是不同的策略。具体地说，个体通过策略 <span class="math inline">\(\mu(a|s)\)</span>生成行为与环境发生实际交互，但是在更新这个状态行为对的价值时使用的是目标策略 <span class="math inline">\(\pi(a|s)\)</span>。目标策略<span class="math inline">\(\pi(a|s)\)</span>多数是已经具备一定能力的策略，例如人类已有的经验或其他个体学习到的经验。借鉴策略学习相当于站在目标策略 <span class="math inline">\(\pi(a|s)\)</span>的“肩膀”上学习。</p><p>​ 基于蒙特卡洛的借鉴策略学习目前认为仅有理论上的研究价值，在实际中用处不大。这里主要讲解常用借鉴策略 TD 学习。</p><p><span class="math display">\[V\left(S_{t}\right) \leftarrow V\left(S_{t}\right)+\alpha\left(\frac{\pi\left(A_{t} | S_{t}\right)}{\mu\left(A_{t} | S_{t}\right)}\left(R_{t+1}+\gamma V\left(S_{t+1}\right)\right)-V\left(S_{t}\right)\right)\]</span> ​ 对于上式，我们可以这样理解：个体处在状态 <span class="math inline">\(S_t\)</span> 中，基于行为策略 µ 产生了一个行为 <span class="math inline">\(A_t\)</span>，执行该行为后进入新的状态 <span class="math inline">\(S_{t+1}\)</span>，借鉴策略学习要做的事情就是，比较借鉴策略和行为策略在状态 <span class="math inline">\(S_t\)</span> 下产生同样的行为 <span class="math inline">\(A_t\)</span> 的概率的比值，如果这个比值接近 1，说明两个策略在状态 <span class="math inline">\(S_t\)</span> 下采取的行为 <span class="math inline">\(A_t\)</span> 的概率差不多，此次对于状态 <span class="math inline">\(S_t\)</span> 价值的更新同时得到两个策略的支持。如果这一概率比值很小，则表明借鉴策略 π 在状态 <span class="math inline">\(S_t\)</span> 下选择 <span class="math inline">\(A_t\)</span> 的机会要小一些，此时为了从借鉴策略学习，我们认为这一步状态价值的更新不是很符合借鉴策略，因而在更新时打些折扣。类似的，如果这个概率比值大于 1，说明按照借鉴策略，选择行为 <span class="math inline">\(A_t\)</span> 的几率要大于当前行为策略产生 <span class="math inline">\(A_t\)</span> 的概率，此时应该对该状态的价值更新就可以大胆些。</p><p>​ 借鉴策略 TD 学习中一个典型的行为策略 µ 是基于行为价值函数 Q(s,a)，ϵ-贪婪策略，借鉴策略 π 则是基于 Q(s，a) 的完全贪婪策略，这种学习方法称为 <strong>Q 学习 (Q learning)</strong>。</p><p>​ Q 学习具体的行为<strong>价值更新公式</strong>：</p><p><span class="math display">\[Q\left(S_{t}, A_{t}\right) \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left(R+\gamma \max _{a^{\prime}} Q\left(S_{t+1}, a^{\prime}\right)-Q\left(S_{t}, A_{t}\right)\right)\]</span> ​ Q 学习的算法流程 ：</p><p><img src="https://cdn.mathpix.com/snip/images/nwg4xl66zibLgArXECOcfFoZ_a_7GiaTHGZPN0oevrQ.original.fullsize.png" /></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;introduction&quot;&gt;1. Introduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;行为策略：指导个体产生与环境进行实际交互行为的策略。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;目标策略：评价状态或行为价值的策略或者待优化的策略。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;现时策略学习：个体在学习过程中优化的策略与自己的行为策略是同一个策略。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;借鉴策略学习：个体在学习过程中优化的策略与自己的行为策略是不同的策略。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="Reinforcement Learning" scheme="http://example.com/categories/Reinforcement-Learning/"/>
    
    
    <category term="学习笔记" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>David Silver Course《Lec04 不基于模型的预测》</title>
    <link href="http://example.com/2021/09/02/notebook04-MC-TD/"/>
    <id>http://example.com/2021/09/02/notebook04-MC-TD/</id>
    <published>2021-09-02T20:18:41.000Z</published>
    <updated>2021-09-03T12:37:27.484Z</updated>
    
    <content type="html"><![CDATA[<h2 id="introduction-简介">1. Introduction 简介</h2><p>​ 如何解决一个可以被认为是 MDP、<strong>但却不掌握MDP 具体细节</strong>的问题，也就是讲述个体如何在没有对环境动力学认识的模型的条件下如何直接通过个体与环境的实际交互来评估一个策略的好坏或者寻找到最优价值函数和最优策略。 本章分为三个部分，将分别从理论上阐述：</p><ul><li><strong>基于完整采样的蒙特卡罗强化学习</strong></li><li><strong>基于不完整采样的时序差分强化学习</strong></li><li><strong>介于两者之间的 λ 时序差分强化学习</strong>。</li></ul><span id="more"></span><h2 id="monte-carlo-learning-蒙特卡罗强化学习">2. Monte-Carlo Learning 蒙特卡罗强化学习</h2><p>​ <strong>蒙特卡罗强化学习</strong> (Monte-Carlo reinforcement learning, MC 学习)： <u>指在不清楚 MDP 状态转移概率的情况下，直接从经历完整的状态序列 (episode) 来估计状态的真实价值，并认为某状态的价值等于在多个状态序列中以该状态算得到的所有收获的平均</u>。</p><p>​ <strong>累进更新平均值（incremental mean)</strong>的计算如下所示： <span class="math display">\[\begin{aligned}\mu_{k} &amp;=\frac{1}{k} \sum_{j=1}^{k} x_{j} \\&amp;=\frac{1}{k}\left(x_{k}+\sum_{j=1}^{k-1} x_{j}\right) \\&amp;=\frac{1}{k}\left(x_{k}+(k-1) \mu_{k-1}\right) \\&amp;=\mu_{k-1}+\frac{1}{k}\left(x_{k}-\mu_{k-1}\right)\end{aligned}\]</span></p><p>​ <strong>递增式的蒙特卡罗法更新状态价值公式</strong>：</p><p><span class="math display">\[\begin{array}{c}N\left(S_{t}\right) \leftarrow N\left(S_{t}\right)+1 \\V\left(S_{t}\right) \leftarrow V\left(S_{t}\right)+\frac{1}{N\left(S_{t}\right)}\left(G_{t}-V\left(S_{t}\right)\right)\end{array}\]</span></p><p>​ 在一些实时或者无法统计准确状态被访问次数时，可以用一个系数 α 来代替状态计数的倒数，此时公式变为：</p><p><span class="math display">\[V\left(S_{t}\right) \leftarrow V\left(S_{t}\right)+\alpha\left(G_{t}-V\left(S_{t}\right)\right)\]</span></p><h2 id="temporal-difference-learning-时序差分强化学习">3. Temporal-Difference Learning 时序差分强化学习</h2><p>​ <strong>时序差分强化学习</strong> (temporal-difference reinforcement learning, TD 学习)：<u>指从采样得到的不完整的状态序列学习，该方法通过合理的引导（bootstrapping），先估计某状态在该状态序列完整后可能得到的收获，并在此基础上利用前文所属的累进更新平均值的方法得到该状态的价值，再通过不断的采样来持续更新这个价值。</u></p><p>​ 具体地说，在 TD 学习中，算法在估计某一个状态的收获时，用的是离开该状态的即刻奖励<span class="math inline">\(R_{t+1}\)</span> 与下一时刻状态 <span class="math inline">\(S_{t+1}\)</span> 的预估状态价值乘以衰减系数<span class="math inline">\(\gamma\)</span>组成：</p><p><span class="math display">\[V\left(S_{t}\right) \leftarrow V\left(S_{t}\right)+\alpha\left(R_{t+1}+\gamma V\left(S_{t+1}\right)-V\left(S_{t}\right)\right)\]</span></p><p>​ 其中<span class="math inline">\(R_{t+1}+\gamma V\left(S_{t+1}\right)\)</span>称为 <strong>TD 目标值</strong> ，<span class="math inline">\(R_{t+1}+\gamma V\left(S_{t+1}\right)-V\left(S_{t}\right)\)</span>称为<strong>TD 误差</strong>。</p><p>​ 引导 (bootstrapping)：指的是用 TD 目标值代替收获<span class="math inline">\(G_t\)</span> 的过程。</p><p>​ <strong>MC与TD算法的区别</strong></p><p>​ TD 学习在更新状态价值时使用的是 TD 目标值，即基于即时奖励和下一状态的预估价值来替代当前状态在状态序列结束时可能得到的收获，它是当前状态价值的有偏估计，而 MC 学习则使用实际的收获来更新状态价值，是某一策略下状态价值的无偏估计。</p><p>​ TD 算法使用了MDP 问题的马儿可夫属性，在具有马尔科夫性的环境下更有效；但是 MC 算法并不利用马儿可夫属性，适用范围不限于具有马尔科夫性的环境。</p><blockquote><p><strong>TD 学习能比 MC 学习更快速灵活的更新状态的价值估计</strong>，这在某些情况下有着非常重要的实际意义。回到驾车返家这个例子中来，我们给驾车返家制定一个新的目标，不再以耗时多少来评估状态价值，而是要求安全平稳的返回家中。假如有一次你在驾车回家的路上突然碰到险情：对面开过来一辆车感觉要和你迎面相撞，严重的话甚至会威胁生命，不过由于最后双方驾驶员都采取了紧急措施没有让险情实际发生，最后平安到家。如果是使用蒙特卡罗学习，路上发生的这一险情可能引发的极大负值奖励将不会被考虑，你不会更新在碰到此类险情时的状态的价值；但是在 TD 学习时，碰到这样的险情过后，你会立即大幅调低这个状态的价值，并在今后再次碰到类似情况时采取其它行为，例如降低速度等来让自身处在一个价值较高的状态中，尽可能避免发生意外事件的发生。</p></blockquote><h2 id="tdlambda">4. TD(<span class="math inline">\(\lambda\)</span>)</h2><p>​ <strong>n-步预测</strong> <u>指从状态序列的当前状态 (<span class="math inline">\(S_t\)</span>) 开始往序列终止状态方向观察至状态<span class="math inline">\(S_{t+n-1}\)</span>，使用这 n 个状态产生的即时奖励 <span class="math inline">\((R_{t+1}; R_{t+2};...;R_{t+n})\)</span> 以及状态<span class="math inline">\(S_{t+n}\)</span> 的预估价值来计算当前第状态<span class="math inline">\(S_t\)</span> 的价值。</u></p><p>​ 定义 <strong>n-步收获</strong>为：</p><p><span class="math display">\[G_{t}^{(n)}=R_{t+1}+\gamma R_{t+2}+\ldots+\gamma^{n-1} R_{t+n}+\gamma^{n} V\left(S_{t+n}\right)\]</span></p><p>​ n-步 TD 学习对应的状态价值函数的<strong>更新公式</strong>为：</p><p><span class="math display">\[V\left(S_{t}\right) \leftarrow V\left(S_{t}\right)+\alpha\left(G_{t}^{(n)}-V\left(S_{t}\right)\right)\]</span></p><p>​ 为了能在不增加计算复杂度的情况下综合考虑所有步数的预测，我们引入了一个新的参数λ，并定义λ-收获为从 n=1 到无穷的所有步收获的权重之和。<br /><span class="math display">\[G_{t}^{\lambda}=(1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_{t}^{(n)}\]</span> ​ 对应的 TD(<span class="math inline">\(\lambda\)</span>) 的更新公式为 :</p><p><span class="math display">\[V\left(S_{t}\right) \leftarrow V\left(S_{t}\right)+\alpha\left(G_{t}^{(\lambda)}-V\left(S_{t}\right)\right)\]</span></p><p>​ 随着n的增大，其 n-收获的权重呈几何级数的衰减。当在 T 时刻到达终止状态时，未分配的权重 (右侧阴影部分) 全部给予终止状态的实际收获值。</p><p>​ <strong>前向认识 TD(<span class="math inline">\(\lambda\)</span>)</strong></p><p>​ 什么是前向认识：<u>更新一个状态的价值需要知道所有后续状态的价值，这和 MC 算法的要求一样，当 λ = 1 时对应的就是 MC 算法，这个实际计算带来了不便。</u></p><p>​ <strong>反向认识 TD(<span class="math inline">\(\lambda\)</span>)</strong></p><p>​ 什么是效用迹：<u>如果把老鼠遭到电击的原因认为是之前接受了较多次数的响铃，则称这种归因为频率启发(frequency heuristic) 式；而把电击归因于最近少数几次状态的影响，则称为就近启发 (recncyheuristic) 式。如果给每一个状态引入一个数值： 效用 (eligibility, E) 来表示该状态对后续状态的影响，就可以同时利用到上述两个启发。而所有状态的效用值总称为效用迹 (eligibility traces,ES)。</u></p><p>​ 效用迹公式：</p><p><span class="math display">\[\begin{array}{c}E_{0}(s)=0 \\E_{t}(s)=\gamma \lambda E_{t-1}(s)+1\left(S_{t}=s\right), \gamma, \lambda \in[0,1]\end{array}\]</span> ​ 针对每一个状态存在一个 E 值，且 E 值并不需要等到状态序列到达终止状态才能计算出来，它是根据已经经过的状态序列来计算得到，并且在每一个时刻都对每一个状态进行一次更新，E 值存在饱和现象，有一个瞬时最高上限：</p><p><span class="math display">\[E_{s a t}=\frac{1}{1-\gamma \lambda}\]</span> ​ 在更新状态价值时把该状态的效用同时考虑进来，价值更新可以表示为 ：</p><p><span class="math display">\[\begin{aligned}\delta_{t}=&amp;\left(R_{t+1}+\gamma V\left(S_{t+1}\right)-V\left(S_{t}\right)\right) \\&amp; V(s) \leftarrow V(s)+\alpha \delta_{t} E_{t}(s)\end{aligned}\]</span></p><ul><li><p>当<span class="math inline">\(\lambda =0\)</span>时，<span class="math inline">\(S_t=s\)</span>一直成立，此时价值更新等同于 TD(0) 算法。</p></li><li><p>当<span class="math inline">\(\lambda =1\)</span>时，可以每经历一个状态就更新状态的价值，这种实时更新的方法并不完全等同于 MC。</p></li><li><p>当<span class="math inline">\(\lambda\in(0,1)\)</span>时，前向认识和反向认识完全等效，但在实时学习时存在差别。</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;introduction-简介&quot;&gt;1. Introduction 简介&lt;/h2&gt;
&lt;p&gt;​ 如何解决一个可以被认为是 MDP、&lt;strong&gt;但却不掌握MDP 具体细节&lt;/strong&gt;的问题，也就是讲述个体如何在没有对环境动力学认识的模型的条件下如何直接通过个体与环境的实际交互来评估一个策略的好坏或者寻找到最优价值函数和最优策略。 本章分为三个部分，将分别从理论上阐述：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;基于完整采样的蒙特卡罗强化学习&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;基于不完整采样的时序差分强化学习&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;介于两者之间的 λ 时序差分强化学习&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="Reinforcement Learning" scheme="http://example.com/categories/Reinforcement-Learning/"/>
    
    
    <category term="学习笔记" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>David Silver Course《Lec03 动态规划》</title>
    <link href="http://example.com/2021/09/02/notebook03-DP/"/>
    <id>http://example.com/2021/09/02/notebook03-DP/</id>
    <published>2021-09-02T20:18:27.000Z</published>
    <updated>2021-09-03T12:37:21.937Z</updated>
    
    <content type="html"><![CDATA[<h2 id="introduction-介绍">1. Introduction 介绍</h2><p>​ <strong>Dynamic</strong>：问题是时序或者顺序的。</p><p>​ <strong>Programming</strong>：规划是指在已知环境动力学（环境的全部状态）的基础上寻找最优策略和最优价值函数</p><p>​ <strong>动态规划的思想</strong>：是将复杂的问题分解为求解子问题，通过求解子问题得到整个问题的解，在解决子问题的时候，其结果通常需要存储起来被用来解决后续复杂问题。</p><p>​ 当问题具有下列两个性质时，通常可以考虑使用动态规划来求解：第一个性质是一个复杂问题的最优解由数个小问题的最优解构成，可以通过寻找子问题的最优解来得到复杂问题的最优解；第二个性质是子问题在复杂问题内重复出现，使得子问题的解可以被存储起来重复利用。</p><p>​ 预测：求解基于某一策略的价值函数 。</p><p>​ 控制：求解最优价值函数和最优策略 。</p><span id="more"></span><h2 id="policy-evaluation-策略评估">2. Policy Evaluation 策略评估</h2><p>​ <strong>策略评估 (policy evaluation) </strong>指计算给定策略下状态价值函数的过程，状态价值函数迭代公式：</p><p><span class="math display">\[v_{k+1}(s)=\sum_{a \in A} \pi(a | s)\left(R_{s}^{a}+\gamma \sum_{s^{\prime} \in S} P_{s s^{\prime}}^{a} v_{k}\left(s^{\prime}\right)\right)\]</span></p><h2 id="policy-iteration-策略迭代">3. Policy Iteration 策略迭代</h2><p>​ 依据新的策略<span class="math inline">\(\pi&#39;\)</span>会得到一个新的价值函数，并产生新的贪婪策略，如此重复循环迭代将最终得到最优价值函数<span class="math inline">\(v^*\)</span> 和最优策略 <span class="math inline">\(\pi^*\)</span>。策略在循环迭代中得到更新改善的过程称为<strong>策略迭代</strong>。</p><p><img src="https://cdn.mathpix.com/snip/images/c25ZEZGONnkh10uvYoGKn2rIMwyLM9EcQ7e6Ammyjow.original.fullsize.png" /></p><p>​ 基于贪婪策略的迭代将收敛于最优策略和最有状态价值函数的证明 ：</p><p><span class="math display">\[\pi^{\prime}(s)=\underset{a \in A}{\operatorname{argmax}} q_{\pi}(s, a)\]</span></p><p>​ 假如个体在与环境交互的仅下一步采取该贪婪策略产生的行为，而在后续步骤仍采取基于原策略产生的行为，那么下面的（不）等式成立 。（确定性策略）</p><p><span class="math display">\[q_{\pi}\left(s, \pi^{\prime}(s)\right)=\max _{a \in A} q_{\pi}(s, a) \geq q_{\pi}(s, \pi(s))=v_{\pi}(s)\]</span> ​ 如果后续状态均使用贪婪策略：</p><p><span class="math display">\[\begin{aligned}v_{\pi}(s) &amp; \leq q_{\pi}\left(s, \pi^{\prime}(s)\right)=\mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) | S_{t}=s\right] \\&amp; \leq \mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma q_{\pi}\left(S_{t+1}, \pi^{\prime}\left(S_{t+1}\right)\right) | S_{t}=s\right] \\&amp; \leq \mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} q_{\pi}\left(S_{t+2}, \pi^{\prime}\left(S_{t+2}\right)\right) | S_{t}=s\right] \\&amp; \leq \mathbb{E}_{\pi^{\prime}}\left[R_{t+1}+\gamma R_{t+2}+\ldots | S_{t}=s\right]=v_{\pi^{\prime}}(s)\end{aligned}\]</span> ​ 如果在某一个迭代周期内，状态价值函数不再改善，即：</p><p><span class="math display">\[q_{\pi}\left(s, \pi^{\prime}(s)\right)=\max _{a \in A} q_{\pi}(s, a)=q_{\pi}(s, \pi(s))=v_{\pi}(s)\]</span> ​ 那么就满足了贝尔曼最优方程的描述：</p><p><span class="math display">\[v_{\pi}=\max _{a \in A} q_{\pi}(s, a)\]</span></p><h2 id="value-iteration-价值迭代">4. Value Iteration 价值迭代</h2><p>​ 一个策略能够获得某状态 s 的最优价值当且仅当该策略也同时获得状态 s 所有可能的后续状态 s′ 的最优价值。一个状态的最优价值可以由其后续状态的最优价值通过前一章所述的贝尔曼最优方程来计算：</p><p><span class="math display">\[v_{*}(s)=\max _{a \in A}\left(R_{s}^{a}+\gamma \sum_{s^{\prime} \in S} P_{s s^{\prime}}^{a} v_{*}\left(s^{\prime}\right)\right)\]</span></p><h2 id="extensions-to-dynamic-programming">5. Extensions to Dynamic Programming</h2><p>​ 异步动态规划算法 ：前文所述的系列算法均为同步动态规划算法，它表示所有的状态更新是同步的。与之对应的还有异步动态规划算法。在这些算法中，每一次迭代并不对所有状态的价值进行更新，而是依据一定的原则有选择性的更新部分状态的价值，这种算法能显著的节约计算资源，并且只要所有状态能够得到持续的被访问更新，那么也能确保算法收敛至最优解。比较常用的异步动态规划思想有：原位动态规划、优先级动态规划、和实时动态规划等。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;introduction-介绍&quot;&gt;1. Introduction 介绍&lt;/h2&gt;
&lt;p&gt;​ &lt;strong&gt;Dynamic&lt;/strong&gt;：问题是时序或者顺序的。&lt;/p&gt;
&lt;p&gt;​ &lt;strong&gt;Programming&lt;/strong&gt;：规划是指在已知环境动力学（环境的全部状态）的基础上寻找最优策略和最优价值函数&lt;/p&gt;
&lt;p&gt;​ &lt;strong&gt;动态规划的思想&lt;/strong&gt;：是将复杂的问题分解为求解子问题，通过求解子问题得到整个问题的解，在解决子问题的时候，其结果通常需要存储起来被用来解决后续复杂问题。&lt;/p&gt;
&lt;p&gt;​ 当问题具有下列两个性质时，通常可以考虑使用动态规划来求解：第一个性质是一个复杂问题的最优解由数个小问题的最优解构成，可以通过寻找子问题的最优解来得到复杂问题的最优解；第二个性质是子问题在复杂问题内重复出现，使得子问题的解可以被存储起来重复利用。&lt;/p&gt;
&lt;p&gt;​ 预测：求解基于某一策略的价值函数 。&lt;/p&gt;
&lt;p&gt;​ 控制：求解最优价值函数和最优策略 。&lt;/p&gt;</summary>
    
    
    
    <category term="Reinforcement Learning" scheme="http://example.com/categories/Reinforcement-Learning/"/>
    
    
    <category term="学习笔记" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>David Silver Course《Lec02 马儿可夫决策过程》</title>
    <link href="http://example.com/2021/09/02/notebook02-MDP/"/>
    <id>http://example.com/2021/09/02/notebook02-MDP/</id>
    <published>2021-09-02T20:18:15.000Z</published>
    <updated>2021-09-03T12:37:16.825Z</updated>
    
    <content type="html"><![CDATA[<h2 id="markov-process-马尔科夫过程">1.Markov Process 马尔科夫过程</h2><h3 id="introduction-to-mdps">1.1 Introduction to MDPs</h3><p>​ <strong>马尔科夫决策过程</strong>：可以对<strong>完全可观测环境</strong>进行描述，几乎所有的强化学习问题都可以转为MDP。</p><span id="more"></span><h3 id="markov-property-马尔科夫特性">1.2 Markov Property 马尔科夫特性</h3><p>​ 马尔科夫特性：The future is independent of the past given the present 。定义：<span class="math inline">\(\mathbb P[S_{t+1}|S_t]=P[S_{t+1}|S_1,...,S_t]\)</span></p><h3 id="state-transition-matrix-状态转移矩阵">1.3 State Transition Matrix 状态转移矩阵</h3><p><span class="math display">\[P_{ss&#39;}=\mathbb P[S_{t+1}=s&#39;|S_t=s]\]</span></p><h3 id="markov-process-马尔科夫过程-1">1.4 Markov Process 马尔科夫过程</h3><p>​ <strong>定义</strong>：马尔可夫过程是一个无记忆的随机过程，即一个随机状态序列具有马尔可夫性。可以用一个元组&lt;S,P&gt;表示，其中S是有限数量的状态集，P是状态转移概率矩阵。</p><p>​ <strong>Sample Episodes：</strong>从起始态C1到结束态Sleep的所有可能的过程。</p><p><img src="https://cdn.mathpix.com/snip/images/Mm0cozScUaYy9QVQI6P_bku4Oenl6SgQ0_bSjbOM6EQ.original.fullsize.png" /></p><h2 id="markov-reward-process-马尔科夫奖励过程">2.Markov Reward Process 马尔科夫奖励过程</h2><h3 id="define-定义">2.1 Define 定义</h3><p>​ A Markov reward process is a Markov chain with values 。定义：马尔科夫奖励过程可以表示为元组<span class="math inline">\(&lt;S,P,R,\gamma&gt;\)</span></p><ul><li><p>S是一个有限状态集合</p></li><li><p>P是状态转移矩阵</p></li><li><p>R是一个奖励函数（reward function），表示在t时状态为s的奖励<span class="math inline">\(R_{t+1}\)</span>。<span class="math inline">\(\mathbb R_s=E[R_{t+1}|S_t=s]\)</span></p><blockquote><p>很多听众纠结为什么奖励是t+1时刻的。照此理解起来相当于离开这个状态才能获得奖励而不是进入这个状态即获得奖励。David指出这仅是一个约定，为了在描述RL问题中涉及到的观测O、行为A、和奖励R时比较方便。他同时指出如果把奖励改为 <span class="math inline">\(R_t\)</span>而不是 <span class="math inline">\(R_{t+1}\)</span>，只要规定好，本质上意义是相同的，在表述上可以把奖励描述为“当进入某个状态会获得相应的奖励”。</p></blockquote></li><li><p><span class="math inline">\(\gamma\)</span>是一个衰减系数（Discount Factor），用来反应更注重现在的reward还是未来的reward。</p></li></ul><h3 id="return-收获">2.2 Return 收获</h3><p>​ <strong>收获<span class="math inline">\(G_t\)</span></strong>：<u>为在一个马尔科夫奖励链上从t时刻开始往后所有的奖励的有衰减的总和，收获是相对于一个样本来说的。</u></p><p><span class="math display">\[G_t=R_{t+1}+\gamma R_{t+2}+...=\sum_{k=0}^{\infty}\gamma^k R_{t+k+1}\]</span></p><p>问：为什么需要衰减因子？</p><ul><li>在数学上是方便的</li><li>防止无限的reward（循环）</li><li>在金融上即时奖励可能比延迟奖励获得更多的利息</li><li>动物/人类的行为表现出对即时回报的偏好</li></ul><h3 id="value-function-价值函数">2.3 Value Function 价值函数</h3><p>​ <strong>价值函数</strong>：<u>在马尔科夫决策过程下基于策略 π 的状态价值函数，表示从状态 s开始，遵循当前策略<span class="math inline">\(\pi\)</span> 时所获得的收获的期望。价值函数是对于一个策略而言的。</u></p><p><span class="math display">\[v(s)=\mathbb E[G_t|S_t=s]\]</span></p><h3 id="bellman-equation-for-mrps-贝尔曼方程">2.4 Bellman Equation for MRPs 贝尔曼方程</h3><p>​ 如下公式，价值函数可以被分解为两个部分：</p><p><span class="math display">\[\begin{aligned}v(s) &amp;=\mathbb{E}\left[G_{t} | S_{t}=s\right] \\&amp;=\mathbb{E}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots | S_{t}=s\right] \\&amp;=\mathbb{E}\left[R_{t+1}+\gamma\left(R_{t+2}+\gamma R_{t+3}+\ldots\right) | S_{t}=s\right] \\&amp;=\mathbb{E}\left[R_{t+1}+\gamma G_{t+1} | S_{t}=s\right] \\&amp;=\mathbb{E}\left[R_{t+1}+\gamma v\left(S_{t+1}\right) | S_{t}=s\right]\end{aligned}\]</span></p><p>​ 一个是该状态的即时奖励期望，也就是即时奖励；另一个是下一时刻状态的价值期望，可以根据下一时刻状态的概率分布得到其期望。如果用<span class="math inline">\(s&#39;\)</span>表示s状态下一时刻任一可能状态，那么贝尔曼方程可写为：</p><p><span class="math display">\[v(s)=R_s+\gamma \sum_{s&#39;\in S}P_{s s&#39;}v(s&#39;)\]</span></p><p>​ 用矩阵可以表示为：<span class="math inline">\(v=R+\gamma P v\)</span>，其中：</p><p><span class="math display">\[\left[\begin{array}{c}v(1) \\\vdots \\v(n)\end{array}\right]=\left[\begin{array}{c}\mathcal{R}_{1} \\\vdots \\\mathcal{R}_{n}\end{array}\right]+\gamma\left[\begin{array}{ccc}\mathcal{P}_{11} &amp; \dots &amp; \mathcal{P}_{1 n} \\\vdots &amp; &amp; \\\mathcal{P}_{n1} &amp; \dots &amp; \mathcal{P}_{n n}\end{array}\right]\left[\begin{array}{c}v(1) \\\vdots \\v(n)\end{array}\right]\]</span></p><pre><code>    求解贝尔曼方程：</code></pre><p><span class="math display">\[\begin{aligned}v &amp;=\mathcal{R}+\gamma \mathcal{P} v \\(I-\gamma \mathcal{P}) v &amp;=\mathcal{R} \\v &amp;=(I-\gamma \mathcal{P})^{-1} \mathcal{R}\end{aligned}\]</span></p><p>对于n个states，该方式复杂度为<span class="math inline">\(O(n^3)\)</span>，只可以对小MRPs方法，对于大规模MRPs问题可以采用：</p><ul><li>Dynamic programming 动态规划</li><li>Monte-Carlo evaluation 蒙特卡洛评价</li><li>Temporal-Difference learning</li></ul><h2 id="markov-decision-process-马尔科夫决策过程">3.Markov Decision Process 马尔科夫决策过程</h2><h3 id="mdp-定义">3.1 MDP 定义</h3><p>​ A Markov decision process (MDP) is a Markov reward process with decisions. It is an environment in which all states are Markov，定义：马尔科夫决策过程可以表示为元组<span class="math inline">\(&lt;S,A,P,R,\gamma&gt;\)</span></p><ul><li>S是一个有限状态集合</li><li>A是一个有限决策集合</li><li>P是状态转移矩阵，<span class="math inline">\(P_{s s&#39;}^a=P[S_{t+1}=s&#39;|S_t=s,A_t=a]\)</span></li><li>R是一个奖励函数（reward function），表示在t时状态为s的奖励<span class="math inline">\(R_{t+1}\)</span>。<span class="math inline">\(\mathbb R_s^a=E[R_{t+1}|S_t=s,A_t=a]\)</span></li><li><span class="math inline">\(\gamma\)</span>是一个衰减系数（Discount Factor），用来反应更注重现在的reward还是未来的reward。</li></ul><h3 id="policies-策略">3.2 Policies 策略</h3><p>​ 策略<span class="math inline">\(\pi\)</span>：<u>是给定一个states，actions的分布。只依赖于当前状态，不依赖于历史。现在只考虑静态的policies，与时间无关。</u></p><p><span class="math display">\[\pi(a|s)=\mathbb P[A_t=a|S_t=s]\]</span></p><p>​ 给定一个MDP <span class="math inline">\(M=&lt;S,A,P,R,\gamma&gt;\)</span>和策略<span class="math inline">\(\pi\)</span>，则状态序列是一个马尔科夫过程<span class="math inline">\(&lt;S,P^\pi&gt;\)</span>，状态和奖励序列是一个马尔科夫奖励过程<span class="math inline">\(&lt;S,P^\pi,R^\pi,\gamma&gt;\)</span> 其中：</p><p><span class="math display">\[\begin{aligned}\mathcal{P}_{s, s^{\prime}}^{\pi} &amp;=\sum_{a \in \mathcal{A}} \pi(a | s) \mathcal{P}_{s s^{\prime}}^{a} \\\mathcal{R}_{s}^{\pi} &amp;=\sum_{a \in \mathcal{A}} \pi(a | s) \mathcal{R}_{s}^{a}\end{aligned}\]</span></p><h3 id="value-function-价值函数-1">3.3 Value Function 价值函数</h3><p>​ <strong>state-value function 状态价值函数</strong><span class="math inline">\(v_\pi(s)\)</span>：<u>表示从状态s开始，<strong>遵循当前策略</strong>时所获得的收获的期望；或者说在执行当前策略π时，衡量个体处在状态s时的价值大小。</u></p><p><span class="math display">\[v_\pi(s)=\mathbb E_\pi[G_t|S_t=s]\]</span></p><p><strong>action-value function 行为价值函数</strong><span class="math inline">\(q_\pi(s,a)\)</span>：<u><strong>表示</strong>在执行策略π时，对当前状态s执行某一具体行为a所能的到的收获的期望；或者说在遵循当前策略π时，衡量对当前状态执行行为a的价值大小。</u> <span class="math display">\[q_\pi(s,a)=\mathbb E_\pi[G_t|S_t=s,A_t=a]\]</span></p><h3 id="bellman-expectation-equation-贝尔曼期望方程">3.4 Bellman Expectation Equation 贝尔曼期望方程</h3><p>可以得出一下两个方程：</p><p><span class="math display">\[v_{\pi}(s)=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) | S_{t}=s\right]\]</span></p><p><span class="math display">\[q_{\pi}(s, a)=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma q_{\pi}\left(S_{t+1}, A_{t+1}\right) | S_{t}=s, A_{t}=a\right]\]</span></p><p>推导：</p><p><span class="math display">\[v_{\pi}(s)=\sum_{a \in \mathcal{A}} \pi(a | s) q_{\pi}(s, a)\]</span></p><p><span class="math display">\[q_{\pi}(s, a)=\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} v_{\pi}\left(s^{\prime}\right)\]</span></p><p><span class="math display">\[v_{\pi}(s)=\sum_{a \in \mathcal{A}} \pi(a | s)\left(\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} v_{\pi}\left(s^{\prime}\right)\right)\]</span></p><p><span class="math display">\[q_{\pi}(s, a)=\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} \sum_{a^{\prime} \in \mathcal{A}} \pi\left(a^{\prime} | s^{\prime}\right) q_{\pi}\left(s^{\prime}, a^{\prime}\right)\]</span></p><p>可得矩阵表达式：<span class="math inline">\(v_{\pi}=\mathcal{R}^{\pi}+\gamma \mathcal{P}^{\pi} v_{\pi}\)</span>，直接解为：<span class="math inline">\(v_{\pi}=\left(I-\gamma \mathcal{P}^{\pi}\right)^{-1} \mathcal{R}^{\pi}\)</span></p><h3 id="optimal-value-function-最优值函数">3.5 Optimal Value Function 最优值函数</h3><p>​ <strong>最优状态值函数<span class="math inline">\(v_∗(s)\)</span></strong>：<u>是所有策略下产生的众多状态价值函数中的最大者：</u></p><p><span class="math display">\[v_{*}(s)=\max _{\pi} v_{\pi}(s)\]</span></p><p>​ <strong>最优行动价值函数<span class="math inline">\(q_*(s,a)\)</span></strong>：<u>是所有策略下产生的众多行为价值函数中的最大者：</u></p><p><span class="math display">\[q_{*}(s,a)=\max _{\pi} q_{\pi}(s,a)\]</span></p><h3 id="optimal-policy-最优策略">3.6 Optimal Policy 最优策略</h3><p>定义一个偏序：<span class="math inline">\(\pi \geq \pi^{\prime}\)</span> if <span class="math inline">\(v_{\pi}(s) \geq v_{\pi^{\prime}}(s), \forall s\)</span> 。<strong>定理</strong> 对于任何MDP，下面几点成立：</p><ol type="1"><li><p>存在一个最优策略，比任何其他策略更好或至少相等；</p></li><li><p>所有的最优策略有相同的最优价值函数；</p></li><li><p>所有的最优策略具有相同的行为价值函数。</p></li></ol><p>可以通过最大化最优行为价值函数来找到最优策略： <span class="math display">\[\pi_{*}(a | s)=\left\{\begin{array}{ll}1 &amp; \text { if } a=\underset{a \in \mathcal{A}}{\operatorname{argmax}} q_{*}(s, a) \\0 &amp; \text { otherwise }\end{array}\right.\]</span></p><p>对于任何MDP问题，总存在一个确定性的最优策略；同时如果我们知道最优行为价值函数，则表明我们找到了最优策略。</p><p>推理： <span class="math display">\[v_{*}(s)=\max _{a} q_{*}(s, a)\\q_{*}(s, a)=\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} v_{*}\left(s^{\prime}\right)\\v_{*}(s)=\max _{a} \mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} v_{*}\left(s^{\prime}\right)\\q_{*}(s, a)=\mathcal{R}_{s}^{a}+\gamma \sum_{s^{\prime} \in \mathcal{S}} \mathcal{P}_{s s^{\prime}}^{a} \max _{a^{\prime}} q_{*}\left(s^{\prime}, a^{\prime}\right)\]</span> Bellman最优方程是非线性的，没有固定的解决方案，通过一些迭代方法来解决：价值迭代、策略迭代、Q学习、Sarsa等。</p><h2 id="extensions-to-mdps">4.Extensions to MDPs</h2><h3 id="infinite-and-continuous-mdps-无限状态或连续mdp">4.1 Infinite and continuous MDPs 无限状态或连续MDP</h3><h3 id="partially-observable-mdps-部分可观测mdp">4.2 Partially observable MDPs 部分可观测MDP</h3><h3 id="undiscounted-average-reward-mdps-非衰减平均奖励mdp">4.3 Undiscounted, average reward MDPs 非衰减、平均奖励MDP</h3>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;markov-process-马尔科夫过程&quot;&gt;1.Markov Process 马尔科夫过程&lt;/h2&gt;
&lt;h3 id=&quot;introduction-to-mdps&quot;&gt;1.1 Introduction to MDPs&lt;/h3&gt;
&lt;p&gt;​ &lt;strong&gt;马尔科夫决策过程&lt;/strong&gt;：可以对&lt;strong&gt;完全可观测环境&lt;/strong&gt;进行描述，几乎所有的强化学习问题都可以转为MDP。&lt;/p&gt;</summary>
    
    
    
    <category term="Reinforcement Learning" scheme="http://example.com/categories/Reinforcement-Learning/"/>
    
    
    <category term="学习笔记" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>David Silver Course《Lec01 强化学习简介》</title>
    <link href="http://example.com/2021/09/02/notebook01-intro-RL/"/>
    <id>http://example.com/2021/09/02/notebook01-intro-RL/</id>
    <published>2021-09-02T20:15:36.000Z</published>
    <updated>2021-09-03T12:37:11.179Z</updated>
    
    <content type="html"><![CDATA[<h2 id="introduction">1. Introduction</h2><p>强化学习在不同领域有不同的表现形式：神经科学、心理学、计算机科学、工程领域、数学、经济学等有不同的称呼。</p><p>强化学习是机器学习的一个分支：监督学习、无监督学习、强化学习</p><p>强化学习的特点：</p><ol type="1"><li>非监督、只有一个奖励信号。</li><li>奖励信号不是实时的，而是延迟的。</li><li>时序是很关键的。所以数据不再是独立同分布的数据。</li><li>当前agent的action影响后续的数据。</li></ol><p>强化学习应用广泛：直升机特技飞行、经典游戏、投资管理、发电站控制、让机器人模仿人类行走等</p><span id="more"></span><h2 id="强化学习问题的提出">2. 强化学习问题的提出</h2><p><strong>奖励 Reward：</strong></p><p>​ reward <span class="math inline">\(R_t\)</span>是一个信号标量，表示agent在t步骤时做的如何，agent的任务就是最大化累积的reward。</p><p>​ 强化学习基于<strong>奖励假设（reward hypothesis）</strong>：所有的目标都可以用期望累积回报的最大化来描述。</p><p><strong>序列决策 Sequential Decision Making ：</strong></p><p>​ 目标：选择actions来最大化未来总体的奖励。</p><p>​ actions是长期的序列，奖励通常是延迟的，有时为了获得更多的长期回报，最好牺牲眼前的回报。</p><p><strong>个体和环境 Agent &amp; Environment</strong></p><p>在t时刻：</p><p>​ agent： 执行<span class="math inline">\(A_t\)</span>，接收<span class="math inline">\(O_t\)</span>，获得<span class="math inline">\(R_t\)</span>。</p><p>​ environment：接收<span class="math inline">\(A_t\)</span>，发出<span class="math inline">\(O_{t+1}\)</span>，发出<span class="math inline">\(R_{t+1}\)</span>。</p><p><img src="https://cdn.mathpix.com/snip/images/r4RoAYgmz3l3-jv-Tn7nSHpSM1yomiF_tWUd40Dd5gI.original.fullsize.png" /></p><p><strong>历史和状态 History &amp; State</strong></p><p>​ 历史是observations、actions、rewards组成的序列，之后的决策应该由历史决定。</p><p><span class="math display">\[H_t=O_1,R_1,A_1,...,A_{t-1},O_t,R_t\]</span> ​ 状态是决定未来发生什么的信息，是历史的一个函数。历史的数据很大且多数对当前决策无用，所以通过历史得出的状态，改状态拥有决定下一步决策的信息。</p><p><span class="math display">\[S_t=f(H_t)\]</span></p><ul><li><p>环境状态 Environment State</p><p>环境状态通常对Agent并不完全可见，也就是Agent有时候并不知道环境状态的所有细节。即使有时候环境状态对Agent可以是完全可见的，这些信息也可能包含着一些无关信息。</p></li><li><p>个体状态 Agent State</p><p>是Agent的内部呈现，包括Agent可以使用的、决定未来动作的所有信息。Agent状态是强化学习算法可以利用的信息，它可以是历史的一个函数：<span class="math inline">\(S_t^a=f(H_t)\)</span>。</p></li><li><p>信息状态 Information State</p><p>包括历史上所有有用的信息，又称Markov状态。状态<span class="math inline">\(S_t\)</span>是马尔可夫的，历史<span class="math inline">\(H_t\)</span>也是马尔可夫的。</p></li></ul><p><strong>完全可观测环境 Fully Observable Environments </strong></p><p>​ 个体能够直接观测到环境状态。在这种条件下：个体对环境的观测 = 个体状态 = 环境状态。正式地说，这种问题是一个马尔科夫决策过程（Markov Decision Process， MDP）</p><p><strong>部分可观测的环境 Partially Observable Environments </strong></p><p>​ 环境是局部可观测的，Agent需要间接观测环境。所以agent state不等于environment state。这种情况被称为：partially observable Markov decision process (POMDP) 。</p><p>​ Agent必须构建自己的state。</p><ul><li>直接采用历史 ：<span class="math inline">\(S_t^a=H_t\)</span></li><li>Beliefs of environment state，采用已知状态的概率分布作为个体的状态：<span class="math inline">\(S_t^a=(P[S_t^e=s^1],...,P[S_t^e=s^n])\)</span></li><li>RNN，根据前一时刻agent的状态和当前的观测得到当前agent的状态：<span class="math inline">\(S_t^a=\sigma(S_{t-1}^aW_s+O_t W_o)\)</span></li></ul><h2 id="inside-an-rl-agent">3. Inside An RL Agent</h2><h3 id="agent的组成部分-major-components-of-an-rl-agent">3.1 Agent的组成部分 Major Components of an RL Agent</h3><p>​ Agent由一下三个的一个或者多个组成。</p><p><strong>（1）策略 Policy</strong></p><p>​ 策略是将state映射为action，分为<strong>确定的策略</strong>（Deterministic policy ）：<span class="math inline">\(a=\pi(s)\)</span>和<strong>随机型策略</strong>（Stochastic policy ）：<span class="math inline">\(\pi(a|s)=P[A_t=1|S_t=S]\)</span></p><p><strong>（2）价值函数 Value Function</strong></p><p>​ 价值函数是对未来回报的预测（期望）。表示通过action进入某一状态评价其好坏程度。</p><p><span class="math display">\[v_{\pi}(s)=\mathbb E_\pi[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+...|S_t=s]\]</span> <strong>（3）模型 Model</strong></p><p>​ 是Agent对环境变化预测的模型，因为Agent希望模拟环境与Agent的交互机制。</p><ul><li>预测下一个可能状态发生的概率 <span class="math inline">\(P_{ss&#39;}^a=\mathbb P[S_{t+1}=s&#39;|S_t=s,A_t=a]\)</span></li><li>预测可能获得的即时奖励：<span class="math inline">\(R_s^a=E[R_{t+1}|S_t=s,A_t=a]\)</span></li></ul><p>​ 模型并不是构建一个个体所必需的，很多强化学习算法中个体并不试图（依赖）构建一个模型。</p><p>注：模型仅针对个体而言，环境实际运行机制不称为模型，而称为<strong>环境动力学</strong>(dynamics of environment)，它能够明确确定个体下一个状态和所得的即时奖励。</p><h3 id="强化学习个体的分类">3.2 强化学习个体的分类</h3><p>可以把个体分为如下三类：</p><ol type="1"><li>仅基于价值函数的 Value Based：在这样的个体中，有对状态的价值估计函数，但是没有直接的策略函数，策略函数由价值函数间接得到。</li><li>仅直接基于策略的 Policy Based：这样的个体中行为直接由策略函数产生，个体并不维护一个对各状态价值的估计函数。</li><li>演员-评判家形式 Actor-Critic：个体既有价值函数、也有策略函数。两者相互结合解决问题。</li></ol><p>此外，根据个体在解决强化学习问题时是否建立一个对环境动力学的模型，将其分为两大类：</p><ol type="1"><li>不基于模型的个体: 这类个体并不视图了解环境如何工作，而仅聚焦于价值和/或策略函数。</li><li>基于模型的个体：个体尝试建立一个描述环境运作过程的模型，以此来指导价值或策略函数的更新。</li></ol><h3 id="学习和规划-learning-planning">3.3 学习和规划 Learning &amp; Planning</h3><ul><li>学习：环境初始时是未知的，个体不知道环境如何工作，个体通过与环境进行交互，逐渐改善其行为策略。</li><li>规划: 环境如何工作对于个体是已知或近似已知的，个体并不与环境发生实际的交互，而是利用其构建的模型进行计算，在此基础上改善其行为策略。</li></ul><p>​ 一个常用的强化学习问题解决思路是，先学习环境如何工作，也就是了解环境工作的方式，即学习得到一个模型，然后利用这个模型进行规划。</p><h3 id="探索和利用-exploration-exploitation">3.4 探索和利用 Exploration &amp; Exploitation</h3><p>​ 强化学习类似于一个试错的学习，个体需要从其与环境的交互中发现一个好的策略，同时又不至于在试错的过程中丢失太多的奖励。探索和利用是个体进行决策时需要平衡的两个方面。</p><h3 id="预测和控制-prediction-control">3.5 预测和控制 Prediction &amp; Control</h3><ul><li>预测：给定一个策略，评价未来。可以看成是求解在给定策略下的价值函数（value function）的过程。How well will I(an agent) do if I(the agent) follow a specific policy?</li><li>控制：找到一个好的策略来最大化未来的奖励。</li></ul><h2 id="课程提纲">4. 课程提纲</h2><h3 id="第一部分强化学习基础理论">第一部分：强化学习基础理论</h3><ol type="1"><li><p>强化学习简介： 本讲</p></li><li><p>马儿可夫决策过程： 理论基础，对于描述强化学习问题很重要</p></li><li><p>动态规划 小规模强化学习问题的一种解决方案</p></li><li><p>不基于模型的预测 理论核心</p></li><li><p>不基于模型的控制 全课重点及核心</p></li></ol><h3 id="第二部分实践中的强化学习">第二部分：实践中的强化学习</h3><ol start="6" type="1"><li><p>价值函数的近似表示 基于价值函数解决大规模问题的常用技巧</p></li><li><p>策略梯度方法 基于策略本身解决大规模问题时的常用技巧</p></li><li><p>整合学习与规划 联合模型解决大规模问题</p></li><li><p>探索和利用 理论介绍如何平衡探索和利用</p></li><li><p>案例学习（选） 强化学习在游戏（博弈）中的应用</p></li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;introduction&quot;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;强化学习在不同领域有不同的表现形式：神经科学、心理学、计算机科学、工程领域、数学、经济学等有不同的称呼。&lt;/p&gt;
&lt;p&gt;强化学习是机器学习的一个分支：监督学习、无监督学习、强化学习&lt;/p&gt;
&lt;p&gt;强化学习的特点：&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;非监督、只有一个奖励信号。&lt;/li&gt;
&lt;li&gt;奖励信号不是实时的，而是延迟的。&lt;/li&gt;
&lt;li&gt;时序是很关键的。所以数据不再是独立同分布的数据。&lt;/li&gt;
&lt;li&gt;当前agent的action影响后续的数据。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;强化学习应用广泛：直升机特技飞行、经典游戏、投资管理、发电站控制、让机器人模仿人类行走等&lt;/p&gt;</summary>
    
    
    
    <category term="Reinforcement Learning" scheme="http://example.com/categories/Reinforcement-Learning/"/>
    
    
    <category term="学习笔记" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读笔记《Fastformer Additive Attention Can Be All You Need》</title>
    <link href="http://example.com/2021/08/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%8AFastformer-Additive-Attention-Can-Be-All-You-Need%E3%80%8B/"/>
    <id>http://example.com/2021/08/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%8AFastformer-Additive-Attention-Can-Be-All-You-Need%E3%80%8B/</id>
    <published>2021-08-30T16:00:00.000Z</published>
    <updated>2021-09-01T03:47:03.683Z</updated>
    
    <content type="html"><![CDATA[<h2 id="summary">0. Summary</h2><p>通过additive attention将query和key矩阵汇总为全局向量，采用element-wise进行交互。得到线性复杂度的模型FastFormer。</p><span id="more"></span><h2 id="problem-statement">1. Problem Statement</h2><p>对于Transformer的二次方计算复杂度，提出更快的attention方法，同时具有较好的效果。</p><h2 id="methods">2. Method(s)</h2><p>整体的框架如下图所示。首先通过additive（这个该怎么翻译）注意力机制将query序列信息汇总为一个全局的query向量。然后将全局query向量与keys通过元素级相乘再通过additive 注意力汇总为一全局key向量。再将全局key向量与value向量元素相乘并通过线性转化为一个学习全局上下文的注意力values。最后与输入query相加。</p><p>总结下来就是将query序列汇总、再与key交互再汇总，再与value交互。中间用到additive attention和element-wise product。后续实验证明element-wise product的作用很大。</p><p><img src="https://i.bmp.ovh/imgs/2021/08/2e7abd5a58efeb89.png" style="zoom: 80%;" /></p><p>1、首先是输入序列通过三个线性转换层得到Q，K，V。</p><p>2、然后通过additive attention汇总query矩阵为一个全局query向量<span class="math inline">\(q\in \mathbb R^{d\times 1}\)</span>（论文中写的<span class="math inline">\(q\in \mathbb R^{d\times d}\)</span>应该是有误），计算方式如下。 <span class="math display">\[\alpha_{i}=\frac{\exp \left(\mathbf{w}_{q}^{T} \mathbf{q}_{i} / \sqrt{d}\right)}{\sum_{j=1}^{N} \exp \left(\mathbf{w}_{q}^{T} \mathbf{q}_{j} / \sqrt{d}\right)}\]</span></p><p><span class="math display">\[q=\sum_{i=1}^N \alpha_i q_i\]</span></p><p>3、然后通过element-wise product将keys与全局的query向量交互。<span class="math inline">\(p_i=q*k_i\)</span>（<span class="math inline">\(*\)</span>表示element-wise product）。再通过additive attention汇总为一个全局key向量。 <span class="math display">\[\beta_{i}=\frac{\exp \left(\mathbf{w}_{k}^{T} \mathbf{p}_{i} / \sqrt{d}\right)}{\sum_{j=1}^{N} \exp \left(\mathbf{w}_{k}^{T} \mathbf{p}_{j} / \sqrt{d}\right)}\]</span></p><p><span class="math display">\[\mathbf{k}=\sum_{i=1}^{N} \beta_{i} \mathbf{p}_{i}\]</span></p><p>4、最后将全局key向量和value进行element-wise product进行交互<span class="math inline">\(u_i=k*v_i\)</span>，再通过线性变换得到输出矩阵<span class="math inline">\(R\)</span>。</p><p>5、最后将输出矩阵<span class="math inline">\(R\)</span>和query矩阵相加为最终的输出。（为什么和query矩阵而不是其他矩阵）。</p><p>参数共享：文章采用了共享query和value的线性变换参数，以及共享不同层之间的参数，以此来减少模型参数。</p><p><strong>复杂度分析：</strong></p><p>additive attention的复杂度为<span class="math inline">\(O(N\cdot d)\)</span>，element-wise product复杂度为<span class="math inline">\(O(N\cdot d)\)</span>，总体复杂度<span class="math inline">\(O(N\cdot d)\)</span>。</p><p>每层参数量为<span class="math inline">\(3hd^2+2hd\)</span>（这是加上了参数共享的），原transformer的参数量为<span class="math inline">\(4hd^2\)</span>。所以说不加上参数共享，参数量并没有减少，但是transformer也可以参数共享的。</p><h2 id="evaluation">3. Evaluation</h2><p>在五个任务上进行了对比实验，对于vanilla Transformer计算成本限制了可以处理的最大序列长度，在截断输入文本序列时丢失了许多有用的上下文，所以效果不如高效的Transformer变种。</p><p>效率对比实验，Fastformer的效率最好。</p><p>交互函数的影响，element-wise product的效果优于concatenate和add。</p><p>参数共享实验，query-value的参数共享和不同层的参数共享不影响效果，但是不同头节点的参数共享影响效果。</p><h2 id="conclusion">4. Conclusion</h2><p>Fastformer，它是一种基于加性注意力的 Transformer 变体，可以有效地以线性复杂度处理长序列。</p><h2 id="notes">5. Notes</h2><h2 id="reference">Reference</h2><p>[1] Wu C, Wu F, Qi T, et al. Fastformer: Additive Attention Can Be All You Need[J]. arXiv preprint arXiv:2108.09084, 2021.</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;summary&quot;&gt;0. Summary&lt;/h2&gt;
&lt;p&gt;通过additive attention将query和key矩阵汇总为全局向量，采用element-wise进行交互。得到线性复杂度的模型FastFormer。&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读" scheme="http://example.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="学习笔记" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Hexo 使用笔记</title>
    <link href="http://example.com/2021/08/26/hexo%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"/>
    <id>http://example.com/2021/08/26/hexo%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/</id>
    <published>2021-08-26T16:00:00.000Z</published>
    <updated>2021-09-01T03:34:35.777Z</updated>
    
    <content type="html"><![CDATA[<p>本地测试：<code>hexo s</code></p><p>发布到GitHub：<code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</code></p><p>生成新文章：<code>hexo new [layout] &lt;title&gt;</code></p><span id="more"></span>]]></content>
    
    
    <summary type="html">&lt;p&gt;本地测试：&lt;code&gt;hexo s&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;发布到GitHub：&lt;code&gt;hexo clean &amp;amp;&amp;amp; hexo g &amp;amp;&amp;amp; hexo d&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;生成新文章：&lt;code&gt;hexo new [layout] &amp;lt;title&amp;gt;&lt;/code&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="Hexo" scheme="http://example.com/categories/Hexo/"/>
    
    
    <category term="学习笔记" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读笔记《REFORMER THE EFFICIENT TRANSFORMER》</title>
    <link href="http://example.com/2021/08/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%8AREFORMER-THE-EFFICIENT-TRANSFORMER%E3%80%8B/"/>
    <id>http://example.com/2021/08/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%E3%80%8AREFORMER-THE-EFFICIENT-TRANSFORMER%E3%80%8B/</id>
    <published>2021-08-25T16:00:00.000Z</published>
    <updated>2021-08-26T16:46:27.178Z</updated>
    
    <content type="html"><![CDATA[<h2 id="summary">0. Summary</h2><p>reformer主要提出的Locality-sensitive hashing attention，根据attention的稀疏和softmax的最大元素支配性质只关心与query最近的K，通过Locality-sensitive hashing实现由query找key，但是受到Q=K的限制。还用到Reversible residual layer降低中间层的内存、以及feed forward层进行Chunking进一步降低显存。</p><span id="more"></span><h2 id="problem-statement">1. Problem Statement</h2><p>Reformer提出了Transformer中的三个问题。</p><ul><li>problem1: 注意力机制的计算需要<span class="math inline">\(O(L^2)\)</span>的时间和空间复杂度。</li><li>problem2: transformer的层数较多，而N层模型的内存消耗是单层模型的N倍，因为需要存储每一层中的激活以进行反向传播。链式法则<span class="math inline">\([g(f(x))]&#39;=g&#39;(f(x))*f&#39;(x)\)</span>。</li><li>problem3: 前馈层的维度通常比注意激活的维度大得多。<span class="math inline">\(d_{ff}&gt;d_{model}\)</span>。在一些模型中<span class="math inline">\(d_{ff}=4K\)</span>甚至更多，需要消耗大量显存。</li></ul><p>针对以上三个问题分别提出了三个解决方法：Locality-sensitive hashing attention、Reversible residual layer、Chunking。</p><h2 id="methods">2. Methods</h2><h3 id="locality-sensitive-hashing-attention">2.1 Locality-sensitive hashing attention</h3><p><strong>hasing attention</strong>：计算和存储全矩阵<span class="math inline">\(QK^T\)</span>是没有必要的，因为我们只对<span class="math inline">\(softmax(QK^T)\)</span>感兴趣，而softmax有最大元素支配的性质。所以对于<span class="math inline">\(q_i\)</span>我们只关心<span class="math inline">\(K\)</span>中与之最接近的前几个<span class="math inline">\(k\)</span>。 <span class="math display">\[\operatorname{Attention}(q_i, K, V)=\operatorname{softmax}\left(\frac{q_i K^{T}}{\sqrt{d_{k}}}\right) V\]</span> <strong>Locality-sensitive hashing</strong>：期望距离近的向量以较高的概率获得相同的散列。哈希大小为b,生成随机的矩阵<span class="math inline">\(R^{[d_k,b/2]}\)</span>。散列函数为<span class="math inline">\(h(x)=argmax([xR;-xR])\)</span></p><p><img src="https://i.bmp.ovh/imgs/2021/08/e3d78151097e7995.png" style="zoom:67%;" /></p><p><strong>LSH attention：</strong>对于LSH attention，Q=K，只需要计算Q和K矩阵的LSH散列，然后仅计算同一哈希桶中的k和q向量的标准关注度。</p><p><strong>LSH attention 流程</strong>：</p><ol type="1"><li>按照桶号对查询进行排序，桶内按照序列位置排序。</li><li>hash桶大小不相同，一个桶中的key和query的数量可能不一样，跨桶批处理困难。为了解决分配不均的问题，令<span class="math inline">\(k=\frac{q}{|q|}\)</span>，这样<span class="math inline">\(h(k)=h(q)\)</span>。论文中对常规的Transformer做了<span class="math inline">\(K = Q\)</span>的实验，证明不影响效果。<span class="math inline">\(K = Q\)</span>带来另一个问题就是通常会更注意自身，可以加一个mask屏蔽掉。</li><li>分块计算:令块的大小<span class="math inline">\(m=\frac{2l}{b}\)</span>，<span class="math inline">\(l\)</span>是序列长度，<span class="math inline">\(b\)</span>是桶的数量。在当前块与前一个块的并中计算权重。</li></ol><p><img src="https://i.bmp.ovh/imgs/2021/08/8e757dc5604f56ab.png" style="zoom:50%;" /></p><h3 id="reversible-residual-layer">2.2 Reversible residual layer</h3><p>reformer中：<span class="math inline">\(Y_1=X_1+Attention(X_2);Y_2=X_2+FeedForward(Y_1)\)</span>，使用可逆残差层而不是标准残差可以在训练过程中仅将激活存储一次，而不是N次。在反向传播时只使用模型参数就可以从下一层的激活结果中恢复任何给定层的激活结果，从而不用保存中间层的激活结果。</p><p><img src="https://s3.bmp.ovh/imgs/2021/08/0391d8362488a0ea.png" style="zoom:50%;" /></p><h3 id="chunking">2.3 Chunking</h3><p>比较厚的层仍会占用大量内存，前馈层的计算在序列中是完全独立的，所以可以分块处理，分chunk分开进行运算。 <span class="math display">\[Y_{2}=\left[Y_{2}^{(1)} ; \ldots ; Y_{2}^{(c)}\right]=\left[X_{2}^{(1)}+\text { FeedForward }\left(Y_{1}^{(1)}\right) ; \ldots ; X_{2}^{(c)}+\text { FeedForward }\left(Y_{1}^{(c)}\right)\right] \]</span></p><h2 id="evaluation">3. Evaluation</h2><p>Shared-QK效果：从下图实验结果可以看出共享QK机制并没有比标准注意力机制效果差。</p><p><img src="https://s3.bmp.ovh/imgs/2021/08/5a8790a42251d1c0.png" /></p><p>可逆层的效果：这里还是用标准Transformer跟可逆网络层对比，二者所使用的参数基本一样，学习曲线图如上：二者曲线基本一致，这说明可逆网络结构在节省内存的前提下，并没有损伤精度。</p><p><img src="https://s3.bmp.ovh/imgs/2021/08/64ace73cb7182c24.png" /></p><p>LSH attention in Transformer：相比全注意力机制，LSH注意力是一个近似的方法，从下面的实验图可以看出随着hash函数的增加，精确度也越来越高。在nrounds = 8的时候，精确度已经跟全注意力机制相匹敌了；但是hash函数越多，计算代价就越高，所以这个超参数可以根据实际计算资源进行调整。</p><p><img src="https://s3.bmp.ovh/imgs/2021/08/f48fcceb6dbe2ba0.png" /></p><p>不同注意力机制的速度：可以看出，随着序列长度的不断增加，标准注意力机制变得越来越慢，而LSH注意力机制基本变化不大，提速效果非常明显。</p><p><img src="https://s3.bmp.ovh/imgs/2021/08/c0d6491f9b0f20ba.png" /></p><h2 id="conclusion">4. Conclusion</h2><p>Reformer 针对 Transformer 中的三个问题提出了三个解决方法Locality-sensitive hashing attention、Reversible residual layer、Chunking，在与 Transformer 模型的性能相当的情况下，降低了在长序列任务下的时间与空间复杂度。</p><h2 id="notes">5. Notes</h2><p>Trax：实现了reformer的过程可以学习<a href="https://github.com/google/trax/tree/master/trax/models/reformer">code</a></p><p>Transformers也对reformer进行了实现<a href="https://github.com/huggingface/transformers">code</a>。</p><h2 id="reference">Reference</h2><p>[1] Kitaev N, Kaiser Ł, Levskaya A. Reformer: The efficient transformer[J]. arXiv preprint arXiv:2001.04451, 2020.</p><p>[2] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[C]//Advances in neural information processing systems. 2017: 5998-6008.</p><p>[3] Beltagy I, Peters M E, Cohan A. Longformer: The long-document transformer[J]. arXiv preprint arXiv:2004.05150, 2020.</p><p>[4] <a href="https://towardsdatascience.com/illustrating-the-reformer-393575ac6ba0">💡Illustrating the Reformer. 🚊 ️ The efficient Transformer | by Alireza Dirafzoon | Towards Data Science</a></p><p>[5] <a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time. (jalammar.github.io)</a></p><p>[6] <a href="https://github.com/huggingface/transformers">huggingface/transformers: 🤗 Transformers: State-of-the-art Natural Language Processing for Pytorch, TensorFlow, and JAX. (github.com)</a></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;summary&quot;&gt;0. Summary&lt;/h2&gt;
&lt;p&gt;reformer主要提出的Locality-sensitive hashing attention，根据attention的稀疏和softmax的最大元素支配性质只关心与query最近的K，通过Locality-sensitive hashing实现由query找key，但是受到Q=K的限制。还用到Reversible residual layer降低中间层的内存、以及feed forward层进行Chunking进一步降低显存。&lt;/p&gt;</summary>
    
    
    
    <category term="论文阅读" scheme="http://example.com/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="学习笔记" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Lec02 渐进符号、递归及解法</title>
    <link href="http://example.com/2021/03/28/Lec02%E6%B8%90%E8%BF%9B%E7%AC%A6%E5%8F%B7%E5%92%8C%E9%80%92%E5%BD%92%E5%8F%8A%E8%A7%A3%E6%B3%95/"/>
    <id>http://example.com/2021/03/28/Lec02%E6%B8%90%E8%BF%9B%E7%AC%A6%E5%8F%B7%E5%92%8C%E9%80%92%E5%BD%92%E5%8F%8A%E8%A7%A3%E6%B3%95/</id>
    <published>2021-03-28T16:00:00.000Z</published>
    <updated>2021-03-28T18:23:47.893Z</updated>
    
    <content type="html"><![CDATA[<p>MIT算法导论课程：Lec02 渐进符号、递归及解法，对应书的章节：Chapters 3-4, excluding section 4.6。</p><span id="more"></span><h2 id="渐进符号asymptotic-notation">1. 渐进符号（Asymptotic Notation）</h2><h3 id="o-notation-upper-bounds">1.1 <span class="math inline">\(O\)</span>-notation （upper bounds）</h3><p><span class="math inline">\(f(n)=O(g(n))\)</span> 如果存在常量<span class="math inline">\(c&gt;0,n_0&gt;0\)</span>对所有的<span class="math inline">\(n\geq n_0\)</span>都有<span class="math inline">\(0\leq f(n)\leq cg(n)\)</span>。</p><p>比如 <span class="math inline">\(2n^2=O(n^3)\)</span>，这个符号可以用<span class="math inline">\(\leq\)</span>理解，表示<span class="math inline">\(g(n)\)</span>是<span class="math inline">\(f(n)\)</span>的上界。</p><h3 id="omega-notationlower-bounds">1.2 <span class="math inline">\(\Omega\)</span>-notation（lower bounds）</h3><p><span class="math inline">\(f(n)=\Omega(g(n))\)</span> 如果存在常量<span class="math inline">\(c&gt;0,n_0&gt;0\)</span>对所有的<span class="math inline">\(n\geq n_0\)</span>都有$0cg(n) f(n) $。</p><p>比如 <span class="math inline">\(\sqrt n=\Omega(\lg n)\)</span>，这个符号可以用<span class="math inline">\(\geq\)</span>理解，表示下界。</p><h3 id="theta-notationtight-bounds">1.3 <span class="math inline">\(\Theta\)</span>-notation（tight bounds）</h3><p><span class="math inline">\(\Theta(g(n))=O(g(n))\cap \Omega(g(n))\)</span></p><p>比如<span class="math inline">\(\frac{1}{2}n^2-2n=\Omega(n^2)\)</span>，可以用<span class="math inline">\(=\)</span>理解。</p><h3 id="o-notation">1.4 <span class="math inline">\(o\)</span>-notation</h3><p><span class="math inline">\(f(n)=o(g(n))\)</span> 如果存在常量<span class="math inline">\(c&gt;0,n_0&gt;0\)</span>对所有的<span class="math inline">\(n\geq n_0\)</span>都有<span class="math inline">\(0\leq f(n)&lt; cg(n)\)</span>。</p><p>比如<span class="math inline">\(2n^2=o(n^3)\)</span>，可以用&lt;理解。</p><h3 id="omega-notation">1.5 <span class="math inline">\(\omega\)</span>-notation</h3><p><span class="math inline">\(f(n)=\omega(g(n))\)</span> 如果存在常量<span class="math inline">\(c&gt;0,n_0&gt;0\)</span>对所有的<span class="math inline">\(n\geq n_0\)</span>都有$0cg(n) &lt; f(n) $。</p><p>比如<span class="math inline">\(\sqrt n=\omega(\lg n)\)</span>，可以用&gt;理解。</p><h2 id="递归recurrences">2. 递归（Recurrences）</h2><h3 id="代换法substitution-method">2.1 代换法（Substitution method）</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. Guess 猜测结果。</span><br><span class="line">2. verify 归纳验证。</span><br><span class="line">3. solve 求系数。</span><br></pre></td></tr></table></figure><p>例子：<span class="math inline">\(T(n)=4T(n/2)+n;T(1)=\Theta(1)\)</span></p><p>（一）假设<span class="math inline">\(T(n)=O(n^3)\)</span></p><ol type="1"><li><p>假设<span class="math inline">\(T(n)=O(n^3)\)</span> 则<span class="math inline">\(T(k)\leq ck^3\)</span> for <span class="math inline">\(k&lt;n\)</span></p></li><li><span class="math inline">\(T(n)=4T(n/2)+n\leq4c(n/2)^3+n=cn^3-((cn^3)/2-n)\)</span> 构造成 desired-residual，就是假设的减去剩下的。</li><li>要使得假设成立，则<span class="math inline">\(cn^3/2-n\geq0\)</span>，所以<span class="math inline">\(c\geq2，n\geq1\)</span>不等式成立。</li><li><p>我们还必须处理初始条件，<span class="math inline">\(T(1)=\Theta(1)\leq c*1^3\)</span>，当c取足够大的数时不等式成立。（This bound is not tight!）</p></li></ol><p>（二）假设<span class="math inline">\(T(n)=O(n^2)\)</span></p><ol type="1"><li>假设<span class="math inline">\(T(n)=O(n^2)\)</span> 则<span class="math inline">\(T(k)\leq ck^2\)</span> for <span class="math inline">\(k&lt;n\)</span></li><li><span class="math inline">\(T(n)=4T(n/2)+n\leq4c(n/2)^2+n=c*n^2-(-n)\)</span></li><li>要使假设成立需要<span class="math inline">\(-n&gt;0\)</span>，不能成立</li></ol><p>（三）加强归纳假设</p><ol type="1"><li>假设<span class="math inline">\(T(n)=O(n^2)\)</span> 则<span class="math inline">\(T(k)\leq c_1k^2-c_2k\)</span> for <span class="math inline">\(k&lt;n\)</span></li><li><span class="math inline">\(T(n)=4T(n/2)+n\leq c_1n^2-c_2n\)</span></li><li>要使假设成立，<span class="math inline">\(c_2\geq 1\)</span></li><li>验证初始条件<span class="math inline">\(T(1)=\Theta(1)\leq c_1*1^2-c_2*1\)</span>，<span class="math inline">\(c_1\)</span>足够大不等式成立。</li></ol><p>求<span class="math inline">\(T(n)\)</span>的下界用类似的方法。</p><h3 id="递归树方法recursion-tree-method">2.2 递归树方法（Recursion-tree method）</h3><p>这个方法不够严谨，比较好的解题过程应该是用递归树法求出结果后再用代换法验证。</p><p><img src="https://ftp.bmp.ovh/imgs/2021/01/33bc3bf17a284dc8.png" style="zoom: 50%;" /></p><h3 id="主方法the-master-method">2.3 主方法（The master method）</h3><p>主方法解决形如<span class="math inline">\(T(n)=aT(n/b)+f(n)\)</span>的式子，并且<span class="math inline">\(a\geq1,b\geq1\)</span>，<span class="math inline">\(f(n)\)</span>是趋近为正的。</p><ol type="1"><li><strong>case1：</strong><span class="math inline">\(f(n)=O(n^{\log_ba-\epsilon}),\epsilon&gt;0\)</span>，<span class="math inline">\(f(n)\)</span> 比<span class="math inline">\(n^{\log_ba}\)</span>的增长慢<span class="math inline">\(n^\epsilon\)</span>多项式级，<strong>Solution：</strong> <span class="math inline">\(T(n)=\Theta(n^{log_ba})\)</span></li><li><strong>case2：</strong> <span class="math inline">\(f(n)=\Theta(n^{\log_ba}\lg^kn),k\geq 0\)</span> <span class="math inline">\(f(n)\)</span>和<span class="math inline">\(n^{log_ba}\)</span>有一样的增长速度，<strong>Solution:</strong><span class="math inline">\(T(n)=\Theta(n^{\log_ba}\lg^{k+1}n)\)</span></li><li><strong>case3：</strong> <span class="math inline">\(f(n)=\Omega(n^{\log_ba+\epsilon}),\epsilon&gt;0\)</span>，并且存在<span class="math inline">\(c&lt;1,af(n/b)\leq cf(n)\)</span> <span class="math inline">\(f(n)\)</span> 比<span class="math inline">\(n^{\log_ba}\)</span>的增长快<span class="math inline">\(n^\epsilon\)</span>多项式级，<strong>solution：</strong> <span class="math inline">\(T(n)=\Theta(f(n))\)</span></li></ol><p><strong>用递归树来理解主方法</strong></p><p><strong>case1:</strong></p><p><img src="https://ftp.bmp.ovh/imgs/2021/01/d96bb64eef94c195.png" style="zoom:50%;" /></p><p><strong>case2：</strong></p><p><img src="https://ftp.bmp.ovh/imgs/2021/01/48ae662c70a10496.png" style="zoom:50%;" /></p><p><strong>case3：</strong></p><p><img src="https://ftp.bmp.ovh/imgs/2021/01/d9e63987e5069e08.png" style="zoom:50%;" /></p><h2 id="appendix-geometric-series">Appendix: geometric series</h2><p><span class="math display">\[\begin{array}{c}1+x+x^{2}+\cdots+x^{n}=\frac{1-x^{n+1}}{1-x} \text { for } x \neq 1 \\1+x+x^{2}+\cdots=\frac{1}{1-x} \quad \text { for }|x|&lt;1\end{array}\]</span></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;MIT算法导论课程：Lec02 渐进符号、递归及解法，对应书的章节：Chapters 3-4, excluding section 4.6。&lt;/p&gt;</summary>
    
    
    
    <category term="算法导论MIT" scheme="http://example.com/categories/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BAMIT/"/>
    
    
    <category term="学习笔记" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Lec01 课程简介及算法分析</title>
    <link href="http://example.com/2021/03/28/Lec01%E8%AF%BE%E7%A8%8B%E7%AE%80%E4%BB%8B%E5%8F%8A%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90/"/>
    <id>http://example.com/2021/03/28/Lec01%E8%AF%BE%E7%A8%8B%E7%AE%80%E4%BB%8B%E5%8F%8A%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90/</id>
    <published>2021-03-28T16:00:00.000Z</published>
    <updated>2021-08-26T16:16:12.359Z</updated>
    
    <content type="html"><![CDATA[<p>MIT算法导论课程：Lec01 简介及算法分析，对应书上的章节：Chapters 1-2。</p><span id="more"></span><h2 id="介绍">1.介绍</h2><h3 id="先修课程">1.1 先修课程</h3><p>离散数学、概率论、编程课程</p><h3 id="算法分析">1.2 算法分析</h3><p>计算机程序性能和资源使用的理论研究（尤其关注性能）。</p><h3 id="比性能更重要的因素">1.3 比性能更重要的因素</h3><p>比如有正确性、简洁性、可维护性、编程成本、稳定性、功能性、模块化、安全性、可扩展性、用户友好度等。</p><h3 id="算法的重要性">1.4 算法的重要性</h3><ul><li>算法将不可行变为可行。</li><li>算法是一种描述程序行为的语言。</li></ul><h2 id="排序问题">2. 排序问题</h2><h3 id="insertion-sort">2.1 Insertion sort</h3><p><strong>伪代码（pseudocode）</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">for j ← 2 to n</span><br><span class="line">    do key ← A[j]</span><br><span class="line">    i ← j-1</span><br><span class="line">    while i &gt; 0 and A[i] &gt;key</span><br><span class="line">        do A[i+1] ← A[i]</span><br><span class="line">        i ← i-1</span><br><span class="line">    A[i+1] ← key</span><br></pre></td></tr></table></figure><p><strong>运行时间（Running time）</strong></p><ul><li>取决于数据，有序的数据速度更快。</li><li>取决于数据规模大小。</li><li>更关注运行时间的上限，更有保证。</li></ul><p><strong>分析方法（Kinds of analyses）</strong></p><ul><li>最坏情况：取决于计算机性能，通常描述不使用绝对速度，关注相对速度。</li><li>平均情况：需要假设数据分布</li><li>最好情况：有迷惑性</li><li>渐近分析（Asymptotic Analysis）<ul><li><span class="math inline">\(\Theta(g(n))={f(n):存在正的常数c_1,c_2已经n_0使得对于所有的n&gt;=n_0都有0\leq c_1g(n)\leq f(n)\leq c_2g(n)}\)</span>，也就是去掉函数的低阶项和最高阶项系数。</li><li>关注随模型增长的增长情况</li></ul></li></ul><h3 id="merge-sort">2.2 Merge Sort</h3><p><strong>伪代码</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MERGE-SORT A[1...n]</span><br><span class="line">1.if n&#x3D;1,done</span><br><span class="line">2.recursively sort A[1..[n&#x2F;2]] and A[[n&#x2F;2]+1,...,n]</span><br><span class="line">3.merge 2 sorted lists</span><br></pre></td></tr></table></figure><p><strong>算法分析</strong></p><p><span class="math inline">\(T(n)=2(T/2)+\Theta(n)\)</span></p><p><span class="math inline">\(T(n)=2(T/2)+c n\)</span></p><p><strong>递归树 Recursion tree</strong></p><p><img src="https://ftp.bmp.ovh/imgs/2021/03/20e766da834288c8.png" style="zoom:67%;" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;MIT算法导论课程：Lec01 简介及算法分析，对应书上的章节：Chapters 1-2。&lt;/p&gt;</summary>
    
    
    
    <category term="算法导论MIT" scheme="http://example.com/categories/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BAMIT/"/>
    
    
    <category term="学习笔记" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Lec12跳跃表</title>
    <link href="http://example.com/2021/03/28/Lec12%E8%B7%B3%E8%B7%83%E8%A1%A8/"/>
    <id>http://example.com/2021/03/28/Lec12%E8%B7%B3%E8%B7%83%E8%A1%A8/</id>
    <published>2021-03-28T08:14:58.000Z</published>
    <updated>2021-03-29T00:23:20.307Z</updated>
    
    <content type="html"><![CDATA[<p>MIT算法导论课程：Lec12跳跃表。</p><span id="more"></span><h2 id="skip-lists">1. Skip Lists</h2><p>跳跃表：随机化动态搜索结构，简单容易实现。<span class="math inline">\(O(\lg n)\)</span>的期望运行时间， 并且有很大的概率<span class="math inline">\(1-\frac{1}{e^\alpha}\)</span>。</p><p>其他高效的动态搜索结构，比如树堆、红黑树、B-trees。</p><p><strong>sorted linked list</strong></p><p>search:<span class="math inline">\(\Theta(n)\)</span> time，如何提速呢？</p><p><strong>Two linked list</strong></p><p>IDEA：新加一个list来作为“快速通道”。</p><p><img src="https://i.bmp.ovh/imgs/2021/03/5c89bcbbdc1a8ea5.png" style="zoom:50%;" /></p><p>search(x)</p><ol type="1"><li>从list L1向右搜索，直到搜索过头</li><li>向下到list L2</li><li>向右搜索，直到搜索到元素</li></ol><p><strong>Analysis</strong></p><p>粗略估计：<span class="math inline">\(|L1|+\frac{|L2|}{|L1|}\)</span>. <span class="math inline">\(|L2|=n\)</span></p><p>最小化： <span class="math inline">\(|L1|=\sqrt n\)</span></p><p>2 sort lists: <span class="math inline">\(2 \sqrt n\)</span></p><p>3 sort lists: <span class="math inline">\(3n^{1/3}\)</span></p><p>k sorted lists: <span class="math inline">\(kn^{1/k}\)</span></p><p><span class="math inline">\(\lg n\)</span> sorted lists:<span class="math inline">\(\lg n*n^{1/\lg n}=2\lg n\)</span></p><p><span class="math inline">\(\lg n\)</span> sorted lists 像一个二分树。</p><p><strong>Ideal skip list</strong> is this <span class="math inline">\(\lg n\)</span> linked list structure.</p><p>上一次与下一层的比为1:2，负无穷存在所有表中的最左边。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">INSERT(x)</span><br><span class="line">最底层插入的有所有元素。以1&#x2F;2的概率决定下一层是否插入。</span><br><span class="line"></span><br><span class="line">DELETE(x)</span><br><span class="line">删除所有表中的x</span><br></pre></td></tr></table></figure><p><img src="https://i.bmp.ovh/imgs/2021/03/614985fbff593b99.png" style="zoom:50%;" /></p><p>不仅平均为<span class="math inline">\(O(\lg n)\)</span>，并且高概率为<span class="math inline">\(O(\lg n)\)</span></p><h2 id="with-high-probability-theorem">2. With-high-probability theorem</h2><p><strong>THEOREM:</strong> 有很高的概率，每次搜索的时间消耗是<span class="math inline">\(O(\lg n)\)</span></p><p><strong>Boole’s inequality</strong></p><p><img src="https://i.bmp.ovh/imgs/2021/03/0aa140691c4a2ad3.png" style="zoom:50%;" /></p><p>如果<span class="math inline">\(k=n^{O(1)}\)</span>，每一个<span class="math inline">\(E_i\)</span>都具有高概率，那么<span class="math inline">\(E_1 \cap E_2,...,\cap E_k\)</span>.</p><p><img src="https://i.bmp.ovh/imgs/2021/03/c19ba3e532edd693.png" style="zoom:67%;" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;MIT算法导论课程：Lec12跳跃表。&lt;/p&gt;</summary>
    
    
    
    <category term="算法导论MIT" scheme="http://example.com/categories/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BAMIT/"/>
    
    
    <category term="学习笔记" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Lec13 平摊分析，表的扩增，势能方法</title>
    <link href="http://example.com/2021/03/28/Lec13%E5%B9%B3%E6%91%8A%E5%88%86%E6%9E%90%E3%80%81%E8%A1%A8%E7%9A%84%E6%89%A9%E5%A2%9E%E3%80%81%E5%8A%BF%E8%83%BD%E6%96%B9%E6%B3%95/"/>
    <id>http://example.com/2021/03/28/Lec13%E5%B9%B3%E6%91%8A%E5%88%86%E6%9E%90%E3%80%81%E8%A1%A8%E7%9A%84%E6%89%A9%E5%A2%9E%E3%80%81%E5%8A%BF%E8%83%BD%E6%96%B9%E6%B3%95/</id>
    <published>2021-03-28T08:14:58.000Z</published>
    <updated>2021-03-29T00:33:03.197Z</updated>
    
    <content type="html"><![CDATA[<p>MIT算法导论课程：Lec13 平摊分析，表的扩增，势能方法，对应书上的章节：Chapter 17</p><p>平摊分析（Amortized Analysis）</p><span id="more"></span><h2 id="dynamic-tables">1. Dynamic tables</h2><p>一个哈希表多大合适？我们知道越大搜索时间越小，但越大浪费空间越多。比较好的大小为<span class="math inline">\(\Theta(n)\)</span>。但如果n是未知的呢？这时可以用动态表解决，动态表的思想是当表溢出时，增大表的大小，就像malloc或new申请一个新的表，然后将旧表的项移动到新表。</p><p><strong>Worst-case analysis</strong></p><p>考虑一个有n个插入操作的序列，在最坏情况下的时间为<span class="math inline">\(n*\Theta(n)=\Theta(n^2)\)</span>，<strong>WRONG!</strong>，因为不是所有操作都是最坏情况。</p><p>定义一个<span class="math inline">\(c_i\)</span>代表第<code>i</code>次插入的代价，<span class="math inline">\(=\left\{\begin{array}{ll} i &amp; \text { if } i-1 \text { is an exact power of } 2 \\ 1 &amp; \text { otherwise } \end{array}\right.\)</span>，表每次扩2倍大。在2的power时进行扩大，代价为<code>i</code>。其他情况下cost为<code>1</code>。 <span class="math display">\[\begin{aligned}\text { Cost of } n \text { insertions } &amp;=\sum_{i=1}^{n} c_{i} \\&amp; \leq n+\sum_{j=0}^{\lfloor\lg (n-1)\rfloor} 2^{j} \\&amp; \leq 3 n \\&amp;=\Theta(n)\end{aligned}\]</span> 因此，dynamic-table操作的平均时间为<span class="math inline">\(\Theta(n)/n=\Theta(1)\)</span>.</p><p><strong>Amortized analysis</strong></p><p>这种分析方法即为平摊分析（Amortized Analysis），用于分析序列操作，然后分摊到每个操作去。没有用的概率，平摊分析可确保在最坏的情况下每个操作的平均性能。</p><p><strong>Types of amortized analyses</strong></p><p>有三种方法：</p><ul><li><p>the aggregate method，</p></li><li>the accounting method,</li><li><p>the potential method</p></li></ul><h2 id="aggregate-method">2. Aggregate method</h2><p>刚刚介绍的是the aggregate method，比较简单。但是其他两种方法允许将特定的摊销成本分配给每个操作。</p><h2 id="accounting-method">3. Accounting method</h2><p>对第<code>i</code>个操作收取一个费用<span class="math inline">\(\hat c_i\)</span>，一个元操作收取一元，执行此操作将消耗此费用。任何未立即消耗的金额都存储在银行中，以供后续操作使用。银行余额不得为负！我们必须确保对于所有的n:<span class="math inline">\(\sum_{i=1}^{n} c_{i} \leq \sum_{i=1}^{n} \hat{c}_{i}\)</span>.因此，总摊销成本为总真实成本提供了一个上限。</p><p><strong>例子</strong></p><p>对<span class="math inline">\(\hat c_i=\$3\)</span>，1个用来支付插入操作，2个用来存入银行。当表扩大的时候，1个钱用来移动一个项。</p><p>银行余额永远不会低于0。因此，摊销成本的总和为真实成本的总和提供了一个上限。</p><p><img src="https://i.bmp.ovh/imgs/2021/03/b6357b14e88b4ae0.png" style="zoom:50%;" /></p><h2 id="potential-method">4. Potential method</h2><p><strong>IDEA：</strong> 将银行帐户视为动态集的势能（按物理原理）</p><p><strong>Framework:</strong></p><ol type="1"><li><p>从初始数据结构<span class="math inline">\(D_0\)</span>开始</p></li><li><p>操作<code>i</code>将<span class="math inline">\(D_{i-1}\)</span>转化为<span class="math inline">\(D_i\)</span>.</p></li><li><p>操作<code>i</code>的cost为<span class="math inline">\(c_i\)</span>.</p></li><li><p>定义势能函数<span class="math inline">\(\Phi :{D_i}-&gt;R,\Phi(D_0)=0 ,\Phi(D_i)&gt;=0\)</span>,</p></li><li><p>平摊代价 <span class="math inline">\(\hat c_i=c_i+\Phi(D_i)-\Phi(D_{i-1})\)</span></p></li></ol><p><img src="https://i.bmp.ovh/imgs/2021/03/7c5c42c3c849922d.png" style="zoom:67%;" /></p><p>n个操作的总平摊成本：<span class="math inline">\(\sum_{i=1}^{n} \hat{c}_{i}=\sum_{i=1}^{n}\left(c_{i}+\Phi\left(D_{i}\right)-\Phi\left(D_{i-1}\right)\right)=\sum_{i=1}^{n}c_i+\Phi(D_n)-\Phi(D_0)\geq \sum_{i=1}^{n} c_i\)</span></p><p><strong>Potential analysis of table doubling</strong></p><p>第<code>i</code>次插入后的势能，定义<span class="math inline">\(\Phi(D_i)=2i-2^{\lceil \lg i \rceil}\)</span>，假设<span class="math inline">\(2^{\lceil \lg i \rceil}=0\)</span></p><p><img src="https://i.bmp.ovh/imgs/2021/03/0be4cc3450eebf77.png" style="zoom: 67%;" /></p><p><img src="https://i.bmp.ovh/imgs/2021/03/6168d499c3f1b09f.png" /></p><p><img src="https://i.bmp.ovh/imgs/2021/03/1c09e78db4040c56.png" /></p><h2 id="小结">5. 小结</h2><ul><li>摊销成本可以提供数据结构性能的清晰抽象。</li><li>当需要进行摊销分析时，可以使用任何一种分析方法，但是每种方法在某些情况下都可以说是最简单或最精确的。</li><li>在accounting方法中分配摊余成本的方法可能不同，在potential方法中分配潜力的方法也可能不同，有时会产生根本不同的界限。</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;MIT算法导论课程：Lec13 平摊分析，表的扩增，势能方法，对应书上的章节：Chapter 17&lt;/p&gt;
&lt;p&gt;平摊分析（Amortized Analysis）&lt;/p&gt;</summary>
    
    
    
    <category term="算法导论MIT" scheme="http://example.com/categories/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BAMIT/"/>
    
    
    <category term="学习笔记" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Lec14 竞争性分析，自组织表</title>
    <link href="http://example.com/2021/03/28/Lec14%E7%AB%9E%E4%BA%89%E6%80%A7%E5%88%86%E6%9E%90%EF%BC%8C%E8%87%AA%E7%BB%84%E7%BB%87%E8%A1%A8/"/>
    <id>http://example.com/2021/03/28/Lec14%E7%AB%9E%E4%BA%89%E6%80%A7%E5%88%86%E6%9E%90%EF%BC%8C%E8%87%AA%E7%BB%84%E7%BB%87%E8%A1%A8/</id>
    <published>2021-03-28T08:14:58.000Z</published>
    <updated>2021-03-29T00:40:29.468Z</updated>
    
    <content type="html"><![CDATA[<p>MIT算法导论课程：Lec14 竞争性分析，自组织表，对应Sleator, Daniel D., and Robert E. Tarjan. &quot;Amortized efficiency of list update and paging rules.&quot; Communications of the ACM 28, no. 2 (February 1985): 202-208.</p><span id="more"></span><h2 id="self-organizing-lists">1. Self-organizing lists</h2><p><strong>Self-organizing lists</strong></p><p>表<code>L</code>有<code>n</code>个元素，操作<code>ACCESS(x)</code>的成本为<span class="math inline">\(rank_L(x)\)</span>，也就是x到头节点的长度。L可以相邻元素的交换，cost为1.</p><p><strong>On-line and off-line problems</strong></p><p>on-line：序列操作<code>S</code>,必须立刻执行操作，不知道未来的操作。</p><p>off-line：可以提前看到之后所有的操作。</p><p>Goal：最小化总成本<span class="math inline">\(C_A(S)\)</span>。</p><p><strong>Worst-case analysis of self organizing lists</strong></p><p>worst case：总是访问L中的尾元素，<span class="math inline">\(C_A(S)=\Omega(|S|*n)\)</span>。</p><p><strong>Average-case analysis of self organizing lists</strong></p><p>假设元素<code>x</code>以概率<code>p(x)</code>被访问，<span class="math inline">\(E[C_A(S)]=\sum_{x\in L}p(x)*rank_L(x)\)</span>。其中当<code>L</code>以概率<code>P</code>降序进行排序时期望成本最小。</p><p>Heuristic:对访问的元素进行计数，然后保存<code>L</code>以计数降序排序。</p><h2 id="move-to-front-heuristic">2. Move-to-front heuristic</h2><p>在实践中发现move-to-front(MTF)启发式的方法有较好的结果。具体做法是在访问<code>x</code>后，将<code>x</code>移动到表头，<span class="math inline">\(cost=2*rank_L(x)\)</span>。MTF方法对于局部性方法有较好的解。</p><h2 id="competitive-analysis-of-mtf">3. Competitive analysis of MTF</h2><p><strong>定义：</strong> 一个on-line算法A是<span class="math inline">\(\alpha-competitive\)</span> 如果存在常数<code>k</code>对于任何操作序列<code>S</code>有<span class="math inline">\(C_A(S)\leq \alpha*C_{OPT}(S)+k\)</span>，其中<span class="math inline">\(OPT\)</span>是off-line的最优算法。</p><p><strong>Theorem：</strong> MTF对于self-organizing lists是一个4-competitive。</p><p><strong>proof：</strong></p><p>首先定义一些变量：</p><p><img src="https://i.bmp.ovh/imgs/2021/03/8d09cdd9f9cd4ce6.png" style="zoom:67%;" /></p><p>定义势能函数<span class="math inline">\(\Phi:{L_i}-&gt;R\)</span> <span class="math display">\[\begin{aligned}&amp;\Phi\left(L_{i}\right)=2 \cdot \mid\left\{(x, y): x \prec_{L_{i}} y \text { and } y \prec_{L_{i} *} x\right\}\\&amp;=2 \cdot \# \text { inversions }\end{aligned}\]</span> <img src="https://i.bmp.ovh/imgs/2021/03/6b13e6ead2c33c68.png" style="zoom:67%;" /></p><p>可以看出：</p><ul><li><span class="math inline">\(\Phi(L_i)\geq 0\)</span> for all i&gt;=0</li><li><span class="math inline">\(\Phi(L_0)=0\)</span> 如果MTF和OPT开始时一样</li></ul><p>一次transpose，<span class="math inline">\(\Delta\Phi=\pm2\)</span>，因为一次transpose会产生或者消除一个逆序对。</p><p>下面看当访问一个元素会发生什么，进行如下的定义：</p><p><img src="https://i.bmp.ovh/imgs/2021/03/3de2caa7c0a77734.png" style="zoom:67%;" /></p><p><span class="math inline">\(r=rank_{i-1}(x)=|A|+|B|+1\)</span>，<span class="math inline">\(r^*=|A|+|C|+1\)</span>.</p><p>MTF将x移动到表头，产生|A|的逆序对，消除了|B|的逆序。对OPT的每次transpose产生小于1的逆序对，因此<span class="math inline">\(\Phi(L_i)-\Phi(L_{i-1}\leq2(|A|-|B|+t_i))\)</span>.</p><p><strong>Amortized cost</strong></p><p>MTF第<code>i</code>次操作的平摊代价为： <span class="math display">\[\begin{aligned}\hat{c}_{i} &amp;=c_{i}+\Phi\left(L_{i}\right)-\Phi\left(L_{i-1}\right) \\&amp; \leq 2 r+2\left(|A|-|B|+t_{i}\right) \\&amp;=2 r+2\left(|A|-(r-1-|A|)+t_{i}\right) \\&amp;=2 r+4|A|-2 r+2+2 t_{i} \\&amp;=4|A|+2+2 t_{i} \\&amp; \leq 4\left(r^{*}+t_{i}\right) \\&amp;=4 c_{i}^{*}\end{aligned}\]</span></p><p>因此我们可以得出：</p><p><span class="math display">\[\begin{aligned}C_{\mathrm{MTF}}(S) &amp;=\sum_{i=1}^{|S|} c_{i} \\&amp;=\sum_{i=1}^{|S|}\left(\hat{c}_{i}+\Phi\left(L_{i-1}\right)-\Phi\left(L_{i}\right)\right) \\&amp; \leq\left(\sum_{i=1}^{|S|} 4 c_{i}^{*}\right)+\Phi\left(L_{0}\right)-\Phi\left(L_{|S|}\right) \\&amp; \leq 4 \cdot C_{\mathrm{OPT}}(S)\end{aligned}\]</span> <strong>Addendum</strong></p><p>如果将把<code>x</code>转移到表头的代价看作0（x从表中拆除在插在表头，时间是常量），则MTF是2-competitive。</p><p>如果<span class="math inline">\(L_0\neq L_0^*\)</span></p><ul><li><span class="math inline">\(\Phi(L_0)\)</span>最差情况为<span class="math inline">\(\Theta(n^2)\)</span>。</li><li>因此：<span class="math inline">\(C_{MTF}\leq 4*C_{OPT}(S)+\Theta(n^2)\)</span>，也是4-competitive，因为<span class="math inline">\(|S|-&gt;+\infty\)</span>，<span class="math inline">\(n^2\)</span>也是常量，n不受到|S|的影响。</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;MIT算法导论课程：Lec14 竞争性分析，自组织表，对应Sleator, Daniel D., and Robert E. Tarjan. &amp;quot;Amortized efficiency of list update and paging rules.&amp;quot; Communications of the ACM 28, no. 2 (February 1985): 202-208.&lt;/p&gt;</summary>
    
    
    
    <category term="算法导论MIT" scheme="http://example.com/categories/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BAMIT/"/>
    
    
    <category term="学习笔记" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Lec16 贪婪算法，最小生成树</title>
    <link href="http://example.com/2021/03/28/Lec16%E8%B4%AA%E5%A9%AA%E7%AE%97%E6%B3%95%EF%BC%8C%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91/"/>
    <id>http://example.com/2021/03/28/Lec16%E8%B4%AA%E5%A9%AA%E7%AE%97%E6%B3%95%EF%BC%8C%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91/</id>
    <published>2021-03-28T08:14:58.000Z</published>
    <updated>2021-03-29T00:35:46.870Z</updated>
    
    <content type="html"><![CDATA[<p>MIT算法导论课程：Lec16 贪婪算法，最小生成树，对应书上的章节：16.1-16.3 and 22.1, Chapter 23</p><span id="more"></span><h2 id="graph-representation">1. Graph representation</h2><p><strong>Graphs</strong></p><p>图分为有向图和无向图，可以用<span class="math inline">\(G=(V,E)\)</span>表示。</p><p><span class="math inline">\(|E|=O(V^2)\)</span>，又因为图是联通的，所以<span class="math inline">\(|E|\geq|V|-1\)</span>，因此<span class="math inline">\(\lg |E|=\Theta(\lg V)\)</span>.</p><p><strong>Adjacency-matrix representation</strong></p><p><img src="https://i.bmp.ovh/imgs/2021/03/4e68a733a02b513e.png" style="zoom:50%;" /></p><p>也可以用权重。<span class="math inline">\(\Theta(V^2)\)</span> storage，dense representation。</p><p><strong>Adjacency-list representation</strong></p><p>用表<span class="math inline">\(Adj[v]\)</span>表示<code>v</code>点指向的节点。</p><p>对于无向图：<span class="math inline">\(|Adj[v]|=degree(v)\)</span>.</p><p>对于有向图：<span class="math inline">\(|Adj[v]|=out-degree(v)\)</span>.</p><p>Handshaking Lemma:对于无向图<span class="math inline">\(\sum degree(v)=2|E|\)</span></p><p>所以Adjacency list是一个sparse representation，<span class="math inline">\(\Theta(V+E)\)</span>，经常比邻接矩阵要好。</p><h2 id="minimum-spanning-trees">2. Minimum spanning trees</h2><p>输入：一个联通无向图<span class="math inline">\(G=(V,E)\)</span>，和权重函数<span class="math inline">\(w:E-&gt;\R\)</span>。为了简化，所有边都是互异的。</p><p>输出：一个生成树<code>T</code>，连接所有节点并且权重最小。<span class="math inline">\(w(T)=\sum_{(u,v)\in T}w(u,v)\)</span>.</p><h2 id="optimal-substructure">3. Optimal substructure</h2><p>移去最小生成树中的任意一个边，然后T被分为了两个子树<span class="math inline">\(T_1\)</span>和<span class="math inline">\(T_2\)</span>.</p><p><strong>Theorem:</strong> 子树<span class="math inline">\(T_1\)</span>是子图<span class="math inline">\(G_1\)</span>的最小生成树，子树<span class="math inline">\(T_2\)</span>是子图<span class="math inline">\(G_2\)</span>的最小生成树。</p><p><strong>Proof：</strong> cut and paste方法：<span class="math inline">\(w(T)=w(u,v)+w(T_1)+w(T_2)\)</span>.</p><p>如果<span class="math inline">\(T_1&#39;\)</span>是比<span class="math inline">\(T_1\)</span>对于<span class="math inline">\(G_1\)</span>更小的生成树这样<span class="math inline">\(T&#39;=\{(u,v)\}\cup T&#39;\cup T_2\)</span>就是比<span class="math inline">\(T\)</span>更小的生成树。这是与假设冲突的。</p><h2 id="greedy-choice">4. Greedy choice</h2><p><strong>Greedy-choice property：</strong>一个局部最优解也是全局最优解。</p><p><strong>Theorem：</strong> <span class="math inline">\(T\)</span>是<span class="math inline">\(G\)</span>的最小生成树，<span class="math inline">\(A\)</span>是<span class="math inline">\(V\)</span>的子集，假设<span class="math inline">\((u,v)\in E\)</span>是<span class="math inline">\(A\)</span>到<span class="math inline">\(V-A\)</span>权重最小的边，那么<span class="math inline">\((u,v)\in T\)</span>.</p><p><strong>Proof:</strong> 假设<span class="math inline">\((u,v)\notin T\)</span>，cut and paste.考虑T中从u到v的唯一简单路径。(u,v)和这条路径的第一个A到V-A的边交换，这样T变得更小了，假设冲突。</p><h2 id="prims-greedy-mst-algorithm">5. Prim’s greedy MST algorithm</h2><p><strong>IDEA:</strong> V-A维护为一个优先队列Q，将A与V之间最小权值边的权值当作Q的key.</p><p><img src="https://i.bmp.ovh/imgs/2021/03/693459b05e1f41f7.png" style="zoom:67%;" /></p><p>不同数据结构时间复杂度对比：</p><p><img src="https://i.bmp.ovh/imgs/2021/03/79b2ab6be27e4594.png" style="zoom:67%;" /></p><p>如今最好算法：</p><ul><li>Karger, Klein, and Tarjan [1993].</li><li>Randomized algorithm.</li><li>O(V + E) expected time</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;MIT算法导论课程：Lec16 贪婪算法，最小生成树，对应书上的章节：16.1-16.3 and 22.1, Chapter 23&lt;/p&gt;</summary>
    
    
    
    <category term="算法导论MIT" scheme="http://example.com/categories/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BAMIT/"/>
    
    
    <category term="学习笔记" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Lec17 最短路径算法：Dijkstra算法，广度优先搜索</title>
    <link href="http://example.com/2021/03/28/Lec17%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E7%AE%97%E6%B3%95%EF%BC%9ADijkstra%E7%AE%97%E6%B3%95%EF%BC%8C%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2/"/>
    <id>http://example.com/2021/03/28/Lec17%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E7%AE%97%E6%B3%95%EF%BC%9ADijkstra%E7%AE%97%E6%B3%95%EF%BC%8C%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2/</id>
    <published>2021-03-28T08:14:58.000Z</published>
    <updated>2021-03-29T00:36:28.570Z</updated>
    
    <content type="html"><![CDATA[<p>MIT算法导论课程：Lec17 最短路径算法：Dijkstra算法，广度优先搜索 ，对应书上的章节：Section 22.2</p><span id="more"></span><h2 id="properties-of-shortest-paths">1. Properties of shortest paths</h2><p><strong>Paths in graphs</strong></p><p>考虑一个带权重边的图，路径的权重为<span class="math inline">\(w(p)=\sum_{i=1}^{k-1}w(v_i,v{i+1})\)</span>.</p><p>从u到v的最短路径为：<span class="math inline">\(\delta(u,v)=min\{w(p:p\ is\ a\ path\ from\ u\ to \ v)\}\)</span>。</p><p>如果<code>u</code>到<code>v</code>不存在路径则<span class="math inline">\(\delta(u,v)=+\infty\)</span>.</p><p>如果边的权重可以为负数，那么最短路径可能不存在，因为可能有一个负数的环。</p><p><strong>Optimal substructure</strong></p><p>Theorem: 最短路径的子路径也是最短路径。</p><p>Proof: Cut and Paste.</p><p><strong>Triangle inequality</strong></p><p>Theorem: 对于所有的<span class="math inline">\(u,v,x,\in V\)</span>，有：<span class="math inline">\(\delta(u,v)\leq \delta(u,x)+\delta(x,v)\)</span>.</p><p><strong>Single-source shortest paths</strong></p><p>假设边的权重都是非负的，所有的最短路径都是存在的。</p><p>IDEA：贪心（Greedy）.</p><ol type="1"><li><p>维护一个从s到该集合S的点的最短路径已知的集合。</p></li><li><p>每一步从<span class="math inline">\(v\in V-S\)</span>中添加一个估计距离s最短点到S中。</p></li><li><p>更新到v的距离。</p></li></ol><h2 id="dijkstras-algorithm">2. Dijkstra’s algorithm</h2><p>算法：</p><p><img src="https://i.bmp.ovh/imgs/2021/03/24742d4fcf59b0a3.png" style="zoom:67%;" /></p><p>松弛操作部分可以理解为对三角不等式的约束进行不断松弛的操作，使之满足，再松弛过程中实现了降序排列Key的操作。</p><h2 id="correctness">3. Correctness</h2><p><strong>Part 1</strong></p><p>Lemma:初始化<span class="math inline">\(d[s]=0,其他的点d[v]=+\infty\)</span>，在整个过程中对于所有<span class="math inline">\(v\in V,d[v]\geq \delta(s,v)\)</span>。</p><p>proof: 反证法，假设<code>v</code>是第一个<span class="math inline">\(d[v]&lt;\delta(s,v)\)</span>的点，<code>u</code>是导致改变的点<span class="math inline">\(d[v]=d[u]+w(u,v)\)</span>。所以有一下矛盾。</p><p><img src="https://i.bmp.ovh/imgs/2021/03/d146f53ed90da685.png" style="zoom:67%;" /></p><p><strong>Part 2</strong></p><p>Lemma：<code>u</code>是<code>v</code>最短路径的前一节点，如果<span class="math inline">\(d[u]=\delta(s,u)\)</span>，并且边<span class="math inline">\((u,v)\)</span>是已经松弛了的，则我们有通过松弛后<span class="math inline">\(d[v]=\delta(s,v)\)</span>。</p><p>proof：松弛过程实现</p><p><strong>Part 3</strong></p><p>Lemma：Dijkstra 算法最后会计算出所有的<span class="math inline">\(d[v]=\delta(s,v)\)</span>.</p><p>Proof：v 添加到S中时可以保证<span class="math inline">\(d[v]=\delta(s,v)\)</span>，通过反证法证明。假设u时第一个添加到S中去<span class="math inline">\(d[u]&gt;\delta(s,u)\)</span>的。假设y是第一个u的最短路径中的S-V中的节点。当x添加时，<span class="math inline">\(d[x]=\delta(s,x)\)</span>，边(x,y)是松弛过的，<span class="math inline">\(d[y]=\delta(s,y)\leq\delta(s,u)&lt;d[u]\)</span>。但是<span class="math inline">\(d[u]\leq d[y]\)</span>，矛盾了。</p><p><img src="https://i.bmp.ovh/imgs/2021/03/7c4193cf01f161eb.png" /></p><h2 id="analysis">4. Analysis</h2><p><img src="https://i.bmp.ovh/imgs/2021/03/e6b587c28a6c44c2.png" style="zoom:67%;" /></p><p><img src="https://i.bmp.ovh/imgs/2021/03/83877cc356c79e83.png" style="zoom:67%;" /></p><h2 id="breadth-first-search">5. Breadth-first search</h2><p>假设对于所有的边<span class="math inline">\(w(u,v)=1\)</span> ，采用FIFO队列代替优先队列。</p><p><img src="https://i.bmp.ovh/imgs/2021/03/df1d68500b8b425f.png" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;MIT算法导论课程：Lec17 最短路径算法：Dijkstra算法，广度优先搜索 ，对应书上的章节：Section 22.2&lt;/p&gt;</summary>
    
    
    
    <category term="算法导论MIT" scheme="http://example.com/categories/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BAMIT/"/>
    
    
    <category term="学习笔记" scheme="http://example.com/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
</feed>
